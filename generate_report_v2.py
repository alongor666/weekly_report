#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½¦é™©å‘¨æŠ¥ç”Ÿæˆå™¨ V2.0 - éº¦è‚¯é”¡çº§è¶‹åŠ¿è¿½è¸ªç‰ˆæœ¬

åŠŸèƒ½å‡çº§:
- âœ… æ™ºèƒ½å‘¨æœŸç®¡ç†(è‡ªåŠ¨æ¨æ–­+æ‰‹åŠ¨è¦†ç›–)
- âœ… å½“å‘¨å‘ç”Ÿå€¼è®¡ç®—(å·®å€¼æ³•+å®¹é”™å¤„ç†)  
- âœ… å¤šæŒ‡æ ‡å› æœåˆ†è§£(èµ”ä»˜ç‡=å‡ºé™©ç‡Ã—æ¡ˆå‡èµ”æ¬¾Ã—æ¡ˆä»¶æ•°)
- âœ… å¼‚å¸¸æ³¢åŠ¨æ£€æµ‹(å•å‘¨æš´æ¶¨/è¿ç»­æ¶åŒ–/è¶‹åŠ¿åè½¬)
- âœ… éº¦è‚¯é”¡çº§æŠ¥å‘Š(é‡‘å­—å¡”åŸç†+So Whatæ€ç»´)
- âœ… æ–°èƒ½æºè´§è½¦ä¸“é¡¹åˆ†æ

è°ƒç”¨é“¾:
data-loader-v2 â†’ kpi-calculator â†’ trend-tracker â†’ 
new-energy-truck â†’ mckinsey-framework â†’ report-assembler
"""

import pandas as pd
import numpy as np
import json
import pickle
from pathlib import Path
from datetime import datetime, timedelta
import re
import warnings
warnings.filterwarnings('ignore')

# =======================================
# V2.0 é…ç½®å‚æ•°
# =======================================

# å‘¨æœŸç®¡ç†é…ç½®
START_WEEK = None      # None = è‡ªåŠ¨æ¨æ–­, æˆ–æ‰‹åŠ¨æŒ‡å®šå¦‚ 35
END_WEEK = None        # None = è‡ªåŠ¨æ¨æ–­, æˆ–æ‰‹åŠ¨æŒ‡å®šå¦‚ 44  
LOOKBACK_WEEKS = 5     # è¶‹åŠ¿åˆ†æå›æº¯å‘¨æ•°

# åˆ†æå¼€å…³
ENABLE_TREND_TRACKING = True      # å¯ç”¨è¶‹åŠ¿è¿½è¸ª
ENABLE_NEW_ENERGY_TRUCK = True    # å¯ç”¨æ–°èƒ½æºè´§è½¦åˆ†æ
ENABLE_MCKINSEY_FRAMEWORK = True  # å¯ç”¨éº¦è‚¯é”¡æ¡†æ¶

# è´¨é‡é˜ˆå€¼
TOLERANCE_MISSING = 0.2      # ç¼ºå¤±æ•°æ®å®¹å¿åº¦
OUTLIER_THRESHOLD = 3.0      # å¼‚å¸¸å€¼æ£€æµ‹é˜ˆå€¼
QUALITY_SCORE_THRESHOLD = 70  # æ•°æ®è´¨é‡è¯„åˆ†é˜ˆå€¼

# è¾“å‡ºé…ç½®
OUTPUT_FOLDER = "å‘¨æŠ¥"
CACHE_FOLDER = ".cache"

# =======================================
# V2.0 æ•°æ®åŠ è½½å™¨ (å¢å¼ºç‰ˆ)
# =======================================

class InsuranceDataLoaderV2:
    """V2.0æ•°æ®åŠ è½½å™¨ - æ™ºèƒ½å‘¨æœŸç®¡ç† + å½“å‘¨å€¼è®¡ç®—"""
    
    def __init__(self, data_folder="å¤„ç†å/"):
        self.data_folder = Path(".")  # æ•°æ®æ–‡ä»¶åœ¨å½“å‰ç›®å½•
        self.available_weeks = []
        self.analysis_period = {}
        
    def detect_available_weeks(self):
        """æ£€æµ‹æ‰€æœ‰å¯ç”¨å‘¨æ¬¡"""
        pattern = "*ä¿å•ç¬¬*å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv"
        self.available_weeks = set()
        
        for csv_file in self.data_folder.glob(pattern):
            match = re.search(r'ç¬¬(\d+)å‘¨', csv_file.name)
            if match:
                week_num = int(match.group(1))
                self.available_weeks.add(week_num)
        
        return sorted(self.available_weeks)
    
    def determine_analysis_period(self, start_week=None, end_week=None, lookback_weeks=5):
        """æ™ºèƒ½ç¡®å®šåˆ†æå‘¨æœŸ"""
        available_weeks = self.detect_available_weeks()
        
        if not available_weeks:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶")
        
        # è‡ªåŠ¨æ¨æ–­é€»è¾‘
        if start_week is None and end_week is None:
            end_week = max(available_weeks)
            start_week = end_week - lookback_weeks + 1
        elif start_week is None:
            start_week = end_week - lookback_weeks + 1
        elif end_week is None:
            end_week = start_week + lookback_weeks - 1
        
        # éªŒè¯å‘¨æœŸ
        analysis_weeks = list(range(start_week, end_week + 1))
        missing_weeks = [w for w in analysis_weeks if w not in available_weeks]
        
        if missing_weeks:
            missing_ratio = len(missing_weeks) / len(analysis_weeks)
            if missing_ratio > TOLERANCE_MISSING:
                raise ValueError(f"ç¼ºå¤±æ•°æ®æ¯”ä¾‹{missing_ratio:.1%}è¶…è¿‡å®¹å¿åº¦")
        
        # ç¡®å®šåŠ è½½å‘¨æ¬¡ï¼ˆåŒ…æ‹¬å‰ä¸€å‘¨ï¼‰
        weeks_to_load = list(range(start_week - 1, end_week + 1))
        weeks_to_load = [w for w in weeks_to_load if w in available_weeks]
        
        self.analysis_period = {
            'start_week': start_week,
            'end_week': end_week,
            'lookback_weeks': lookback_weeks,
            'analysis_weeks': analysis_weeks,
            'weeks_to_load': weeks_to_load,
            'missing_weeks': missing_weeks,
            'missing_ratio': len(missing_weeks) / len(analysis_weeks) if analysis_weeks else 0
        }
        
        return self.analysis_period
    
    def load_data_files(self, weeks_to_load):
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        loaded_data = {}
        load_errors = []
        
        for week in weeks_to_load:
            pattern = f"*ä¿å•ç¬¬{week}å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv"
            matching_files = list(self.data_folder.glob(pattern))
            
            if not matching_files:
                load_errors.append(f"ç¬¬{week}å‘¨: æœªæ‰¾åˆ°æ–‡ä»¶")
                continue
            
            week_data = []
            for file in matching_files:
                try:
                    df = pd.read_csv(file, encoding='utf-8-sig')
                    df['week_number'] = week
                    df['data_source'] = file.name
                    week_data.append(df)
                except Exception as e:
                    print(f"âŒ åŠ è½½ {file.name} å¤±è´¥: {e}")
                    load_errors.append(f"ç¬¬{week}å‘¨: {str(e)}")
            
            if week_data:
                combined_df = pd.concat(week_data, ignore_index=True)
                loaded_data[week] = combined_df
                print(f"âœ… ç¬¬{week}å‘¨: æˆåŠŸåŠ è½½ {len(combined_df)} è¡Œæ•°æ®")
        
        return loaded_data, load_errors
    
    def preprocess_data(self, loaded_data):
        """æ•°æ®é¢„å¤„ç†"""
        for week, df in loaded_data.items():
            print(f"\nğŸ”§ é¢„å¤„ç†ç¬¬{week}å‘¨æ•°æ®...")
            
            # è¿‡æ»¤æœ¬éƒ¨
            original_count = len(df)
            df_filtered = df[df['third_level_organization'] != 'æœ¬éƒ¨'].copy()
            
            # å¹´åº¦åˆ†ç»„
            df_filtered['policy_year'] = df_filtered['policy_start_year'].astype(str).str.extract(r'(202[45])')[0]
            
            # æ•°æ®ç±»å‹æ ‡å‡†åŒ–
            numeric_cols = ['signed_premium_yuan', 'matured_premium_yuan', 'reported_claim_payment_yuan', 
                           'expense_amount_yuan', 'claim_case_count']
            for col in numeric_cols:
                df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce').fillna(0)
            
            loaded_data[week] = df_filtered
            print(f"  - è¿‡æ»¤å: {len(df_filtered)} è¡Œæ•°æ®")
        
        return loaded_data
    
    def calculate_weekly_values(self, loaded_data, analysis_weeks):
        """è®¡ç®—å½“å‘¨å‘ç”Ÿå€¼"""
        weekly_data = {}
        
        for week in analysis_weeks:
            if week not in loaded_data:
                continue
                
            current_df = loaded_data[week]
            previous_week = week - 1
            previous_df = loaded_data.get(previous_week)
            
            weekly_values = None
            if previous_df is not None:
                weekly_values = self._calculate_weekly_metrics(current_df, previous_df)
            
            # æŒ‰å¹´åº¦åˆ†ç»„
            for year in ['2024', '2025']:
                year_current = current_df[current_df['policy_year'] == year]
                
                if len(year_current) > 0:
                    if year not in weekly_data:
                        weekly_data[year] = {'cumulative': {}, 'weekly': {}}
                    
                    weekly_data[year]['cumulative'][week] = year_current
                    
                    if weekly_values is not None:
                        year_weekly = weekly_values[weekly_values['policy_year'] == year]
                        if len(year_weekly) > 0:
                            weekly_data[year]['weekly'][week] = year_weekly
        
        return weekly_data
    
    def _calculate_weekly_metrics(self, current_df, previous_df):
        """è®¡ç®—å…·ä½“å½“å‘¨æŒ‡æ ‡"""
        # æ±‡æ€»çº§åˆ«è®¡ç®—
        current_summary = current_df.groupby('policy_year').agg({
            'signed_premium_yuan': 'sum',
            'matured_premium_yuan': 'sum',
            'reported_claim_payment_yuan': 'sum',
            'claim_case_count': 'sum',
            'expense_amount_yuan': 'sum'
        }).reset_index()
        
        previous_summary = previous_df.groupby('policy_year').agg({
            'signed_premium_yuan': 'sum',
            'matured_premium_yuan': 'sum',
            'reported_claim_payment_yuan': 'sum',
            'claim_case_count': 'sum',
            'expense_amount_yuan': 'sum'
        }).reset_index()
        
        # è®¡ç®—å½“å‘¨å€¼
        weekly_summary = current_summary.copy()
        for col in ['signed_premium_yuan', 'matured_premium_yuan', 'reported_claim_payment_yuan', 'expense_amount_yuan']:
            weekly_summary[col] = current_summary[col] - previous_summary[col]
        
        # è®¡ç®—æ¡ˆå‡èµ”æ¬¾
        weekly_summary['avg_claim_amount'] = (
            weekly_summary['reported_claim_payment_yuan'] / 
            weekly_summary['claim_case_count'].replace(0, 1)
        )
        
        return weekly_summary

# =======================================
# V2.0 KPIè®¡ç®—å™¨ (è¶‹åŠ¿å¢å¼ºç‰ˆ)
# =======================================

class InsuranceKpiCalculatorV2:
    """V2.0 KPIè®¡ç®—å™¨ - æ”¯æŒè¶‹åŠ¿åˆ†æå’Œå¼‚å¸¸æ£€æµ‹"""
    
    def __init__(self):
        self.kpi_definitions = self._load_kpi_definitions()
    
    def _load_kpi_definitions(self):
        """åŠ è½½KPIå®šä¹‰å’Œé˜ˆå€¼"""
        return {
            'æ»¡æœŸèµ”ä»˜ç‡': {'field': 'reported_claim_payment_yuan', 'base': 'matured_premium_yuan', 'format': 'ratio'},
            'æ»¡æœŸè¾¹é™…è´¡çŒ®ç‡': {'calculation': 'complex', 'format': 'ratio'},
            'è´¹ç”¨ç‡': {'field': 'expense_amount_yuan', 'base': 'signed_premium_yuan', 'format': 'ratio'},
            'å‡ºé™©ç‡': {'field': 'claim_case_count', 'base': 'policy_count', 'format': 'ratio'},
            'æ¡ˆå‡èµ”æ¬¾': {'field': 'reported_claim_payment_yuan', 'base': 'claim_case_count', 'format': 'amount'},
            'å•å‡ä¿è´¹': {'field': 'signed_premium_yuan', 'base': 'policy_count', 'format': 'amount'}
        }
    
    def calculate_kpis(self, df, period_type='cumulative'):
        """è®¡ç®—KPIæŒ‡æ ‡"""
        if len(df) == 0:
            return {}
        
        # åŸºç¡€æŒ‡æ ‡è®¡ç®—
        total_premium = df['signed_premium_yuan'].sum()
        matured_premium = df['matured_premium_yuan'].sum()
        total_claims = df['reported_claim_payment_yuan'].sum()
        total_expenses = df['expense_amount_yuan'].sum()
        claim_cases = df['claim_case_count'].sum() if 'claim_case_count' in df.columns else 0
        policy_count = df['policy_count'].sum() if 'policy_count' in df.columns else 0
        
        # æ ¸å¿ƒKPIè®¡ç®—
        loss_ratio = (total_claims / (matured_premium + 1)) * 100
        expense_ratio = (total_expenses / (total_premium + 1)) * 100
        variable_cost_ratio = loss_ratio + expense_ratio
        contribution_margin = 100 - variable_cost_ratio
        
        claim_rate = (claim_cases / (policy_count + 1)) * 100
        avg_claim = total_claims / (claim_cases + 1)
        avg_premium = total_premium / (policy_count + 1)
        
        kpis = {
            'period_type': period_type,
            'æ»¡æœŸä¿è´¹': round(matured_premium / 10000, 2),
            'ç­¾å•ä¿è´¹': round(total_premium / 10000, 2),
            'å·²æŠ¥å‘Šèµ”æ¬¾': round(total_claims / 10000, 2),
            'è´¹ç”¨æ€»é¢': round(total_expenses / 10000, 2),
            'æ»¡æœŸèµ”ä»˜ç‡': round(loss_ratio, 2),
            'æ»¡æœŸè¾¹é™…è´¡çŒ®ç‡': round(contribution_margin, 2),
            'è´¹ç”¨ç‡': round(expense_ratio, 2),
            'å˜åŠ¨æˆæœ¬ç‡': round(variable_cost_ratio, 2),
            'å‡ºé™©ç‡': round(claim_rate, 2),
            'æ¡ˆå‡èµ”æ¬¾': round(avg_claim, 0),
            'å•å‡ä¿è´¹': round(avg_premium, 0),
            'ä¿å•ä»¶æ•°': int(policy_count),
            'èµ”æ¡ˆä»¶æ•°': int(claim_cases)
        }
        
        # çŠ¶æ€åˆ¤æ–­
        kpis['èµ”ä»˜ç‡çŠ¶æ€'] = self._judge_loss_ratio_status(loss_ratio)
        kpis['è¾¹è´¡ç‡çŠ¶æ€'] = self._judge_contribution_status(contribution_margin)
        
        return kpis
    
    def calculate_trend_kpis(self, weekly_data):
        """è®¡ç®—è¶‹åŠ¿KPI"""
        trend_analysis = {}
        
        for year in weekly_data.keys():
            if 'weekly' not in weekly_data[year]:
                continue
                
            weekly_values = weekly_data[year]['weekly']
            if not weekly_values:
                continue
            
            trend_analysis[year] = {
                'weekly_kpis': {},
                'trend_insights': {}
            }
            
            # é€å‘¨è®¡ç®—KPI
            for week, df in weekly_values.items():
                if len(df) > 0:
                    kpis = self.calculate_kpis(df, period_type='weekly')
                    trend_analysis[year]['weekly_kpis'][week] = kpis
            
            # è¶‹åŠ¿æ´å¯Ÿ
            trend_analysis[year]['trend_insights'] = self._analyze_trends(trend_analysis[year]['weekly_kpis'])
        
        return trend_analysis
    
    def _judge_loss_ratio_status(self, loss_ratio):
        """åˆ¤æ–­èµ”ä»˜ç‡çŠ¶æ€"""
        if loss_ratio < 50:
            return "ä¼˜ç§€"
        elif loss_ratio < 60:
            return "è‰¯å¥½"
        elif loss_ratio < 70:
            return "ä¸­ç­‰"
        elif loss_ratio < 80:
            return "é¢„è­¦"
        else:
            return "å±é™©"
    
    def _judge_contribution_status(self, contribution_margin):
        """åˆ¤æ–­è¾¹é™…è´¡çŒ®ç‡çŠ¶æ€"""
        if contribution_margin > 12:
            return "ä¼˜ç§€"
        elif contribution_margin > 8:
            return "è‰¯å¥½"
        elif contribution_margin > 6:
            return "ä¸­ç­‰"
        elif contribution_margin > 4:
            return "é¢„è­¦"
        else:
            return "å±é™©"
    
    def _analyze_trends(self, weekly_kpis):
        """åˆ†æè¶‹åŠ¿ç‰¹å¾"""
        if len(weekly_kpis) < 3:
            return {"insufficient_data": True}
        
        weeks = sorted(weekly_kpis.keys())
        loss_ratios = [weekly_kpis[w]['æ»¡æœŸèµ”ä»˜ç‡'] for w in weeks]
        claim_amounts = [weekly_kpis[w]['æ¡ˆå‡èµ”æ¬¾'] for w in weeks]
        claim_rates = [weekly_kpis[w]['å‡ºé™©ç‡'] for w in weeks]
        
        insights = {
            'loss_ratio_trend': self._calculate_trend(loss_ratios),
            'avg_claim_trend': self._calculate_trend(claim_amounts),
            'claim_rate_trend': self._calculate_trend(claim_rates),
            'volatility_analysis': self._analyze_volatility(loss_ratios)
        }
        
        # å¼‚å¸¸æ£€æµ‹
        insights['anomalies'] = self._detect_anomalies(weekly_kpis)
        
        return insights
    
    def _calculate_trend(self, values):
        """è®¡ç®—è¶‹åŠ¿"""
        if len(values) < 2:
            return {"direction": "stable", "change": 0}
        
        # ç®€å•çº¿æ€§è¶‹åŠ¿
        x = list(range(len(values)))
        slope = np.polyfit(x, values, 1)[0]
        
        avg_change = (values[-1] - values[0]) / len(values)
        
        if slope > 0.5:
            direction = "ä¸Šå‡"
        elif slope < -0.5:
            direction = "ä¸‹é™"
        else:
            direction = "ç¨³å®š"
        
        return {
            "direction": direction,
            "slope": round(slope, 3),
            "avg_weekly_change": round(avg_change, 2)
        }
    
    def _analyze_volatility(self, values):
        """åˆ†ææ³¢åŠ¨æ€§"""
        if len(values) < 3:
            return {}
        
        std_dev = np.std(values)
        mean_val = np.mean(values)
        cv = std_dev / mean_val if mean_val != 0 else 0
        
        return {
            "standard_deviation": round(std_dev, 2),
            "coefficient_variation": round(cv, 3),
            "volatility_level": "é«˜" if cv > 0.1 else "ä¸­" if cv > 0.05 else "ä½"
        }
    
    def _detect_anomalies(self, weekly_kpis):
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []
        weeks = sorted(weekly_kpis.keys())
        
        for i, week in enumerate(weeks):
            current = weekly_kpis[week]
            
            # æ£€æµ‹æ¡ˆå‡èµ”æ¬¾å¼‚å¸¸
            if i > 0:
                prev_avg = weekly_kpis[weeks[i-1]]['æ¡ˆå‡èµ”æ¬¾']
                curr_avg = current['æ¡ˆå‡èµ”æ¬¾']
                
                if prev_avg > 0 and curr_avg > prev_avg * 1.5:
                    anomalies.append({
                        "week": week,
                        "type": "æ¡ˆå‡èµ”æ¬¾çªå¢",
                        "value": curr_avg,
                        "previous": prev_avg,
                        "change_ratio": round((curr_avg - prev_avg) / prev_avg, 2)
                    })
        
        return anomalies

# =======================================
# V2.0 è¶‹åŠ¿è¿½è¸ªå™¨ (éº¦è‚¯é”¡çº§)
# =======================================

class InsuranceLossTrendTrackerV2:
    """V2.0è¶‹åŠ¿è¿½è¸ªå™¨ - éº¦è‚¯é”¡çº§ä¸“ä¸šåˆ†æ"""
    
    def __init__(self):
        self.risk_thresholds = {
            'loss_ratio': {'excellent': 60, 'good': 70, 'warning': 80},
            'trend_slope': {'improving': -1, 'stable': 1, 'deteriorating': 1}
        }
    
    def analyze_trends(self, data_by_year, trend_kpis):
        """ä¸»åˆ†æå‡½æ•°"""
        trend_report = {}
        
        for year in trend_kpis.keys():
            if 'weekly_kpis' not in trend_kpis[year]:
                continue
            
            weekly_kpis = trend_kpis[year]['weekly_kpis']
            trend_insights = trend_kpis[year]['trend_insights']
            
            trend_report[year] = {
                'executive_summary': self._generate_executive_summary(weekly_kpis, trend_insights),
                'problem_organizations': self._identify_problem_orgs(data_by_year[year], weekly_kpis),
                'anomaly_analysis': self._deep_anomaly_analysis(weekly_kpis),
                'strategic_recommendations': self._generate_mckinsey_recommendations(trend_insights)
            }
        
        return trend_report
    
    def _generate_executive_summary(self, weekly_kpis, trend_insights):
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦ (éº¦è‚¯é”¡é‡‘å­—å¡”)"""
        if not weekly_kpis:
            return {"insufficient_data": True}
        
        weeks = sorted(weekly_kpis.keys())
        latest_week = weeks[-1]
        latest_kpi = weekly_kpis[latest_week]
        
        # æ ¸å¿ƒç»“è®º
        core_conclusion = self._formulate_core_conclusion(latest_kpi, trend_insights)
        
        # 3ä¸ªå…³é”®æ”¯æ’‘
        key_supports = [
            f"èµ”ä»˜ç‡{latest_kpi['æ»¡æœŸèµ”ä»˜ç‡']}% ({latest_kpi['èµ”ä»˜ç‡çŠ¶æ€']})",
            f"è¶‹åŠ¿{trend_insights.get('loss_ratio_trend', {}).get('direction', 'æœªçŸ¥')}",
            f"æ¡ˆå‡èµ”æ¬¾{latest_kpi['æ¡ˆå‡èµ”æ¬¾']:,.0f}å…ƒ"
        ]
        
        # ç«‹å³è¡ŒåŠ¨å»ºè®®
        immediate_action = self._determine_immediate_action(latest_kpi, trend_insights)
        
        return {
            "core_conclusion": core_conclusion,
            "key_supports": key_supports,
            "immediate_action": immediate_action,
            "latest_week": latest_week,
            "latest_metrics": latest_kpi
        }
    
    def _identify_problem_orgs(self, year_data, weekly_kpis):
        """è¯†åˆ«é—®é¢˜æœºæ„ (ä¸‰å±‚ä¸‹é’»)"""
        problem_orgs = []
        
        # è·å–æœ€æ–°å‘¨çš„æ•°æ®
        if 'cumulative' not in year_data or not year_data['cumulative']:
            return problem_orgs
            
        latest_week = max(year_data['cumulative'].keys())
        latest_data = year_data['cumulative'][latest_week]
        
        # Layer 1: æ‰¾å‡ºTOP3é—®é¢˜æœºæ„
        org_analysis = self._analyze_by_organization(latest_data)
        top3_problem_orgs = sorted(org_analysis, key=lambda x: x['risk_score'])[:3]
        
        for org_info in top3_problem_orgs:
            org_name = org_info['organization']
            org_df = latest_data[latest_data['third_level_organization'] == org_name]
            
            # Layer 2: æ‰¾å‡ºTOP3é—®é¢˜ä¸šåŠ¡
            business_analysis = self._analyze_by_business_type(org_df)
            top3_business = sorted(business_analysis, key=lambda x: x['impact_score'], reverse=True)[:3]
            
            # Layer 3: æ‰¾å‡ºæœ€å·®é™©åˆ«ç»„åˆ
            for business_info in top3_business:
                business_type = business_info['business_type']
                business_df = org_df[org_df['business_type_category'] == business_type]
                
                coverage_analysis = self._analyze_by_coverage(business_df)
                worst_coverage = max(coverage_analysis, key=lambda x: x['loss_ratio']) if coverage_analysis else None
                
                business_info['worst_coverage'] = worst_coverage
            
            org_info['problem_businesses'] = top3_business
            problem_orgs.append(org_info)
        
        return problem_orgs
    
    def _analyze_by_organization(self, df):
        """æŒ‰æœºæ„åˆ†æ"""
        org_analysis = []
        
        for org in df['third_level_organization'].unique():
            if pd.isna(org):
                continue
                
            org_df = df[df['third_level_organization'] == org]
            
            if len(org_df) == 0:
                continue
            
            kpis = self._calculate_org_kpis(org_df)
            risk_score = self._calculate_risk_score(kpis)
            
            org_analysis.append({
                'organization': org,
                'risk_score': risk_score,
                'kpis': kpis,
                'premium_scale': org_df['matured_premium_yuan'].sum() / 10000
            })
        
        return org_analysis
    
    def _calculate_org_kpis(self, org_df):
        """è®¡ç®—æœºæ„KPI"""
        total_premium = org_df['matured_premium_yuan'].sum()
        total_claims = org_df['reported_claim_payment_yuan'].sum()
        loss_ratio = (total_claims / (total_premium + 1)) * 100
        
        return {
            'loss_ratio': loss_ratio,
            'premium_scale': total_premium / 10000,
            'claim_cases': org_df['claim_case_count'].sum(),
            'policies': org_df['policy_count'].sum()
        }
    
    def _calculate_risk_score(self, kpis):
        """è®¡ç®—é£é™©è¯„åˆ† (0-100, è¶Šä½è¶Šå·®)"""
        loss_ratio = kpis['loss_ratio']
        
        # åŸºç¡€çŠ¶æ€åˆ†
        if loss_ratio < 60:
            status_score = 97.5
        elif loss_ratio < 70:
            status_score = 90
        elif loss_ratio < 75:
            status_score = 77.5
        elif loss_ratio < 80:
            status_score = 54.5
        else:
            status_score = 19.5
        
        return status_score
    
    def _analyze_by_business_type(self, org_df):
        """æŒ‰ä¸šåŠ¡ç±»å‹åˆ†æ"""
        business_analysis = []
        
        for business_type in org_df['business_type_category'].unique():
            if pd.isna(business_type):
                continue
                
            business_df = org_df[org_df['business_type_category'] == business_type]
            
            if business_df['matured_premium_yuan'].sum() < org_df['matured_premium_yuan'].sum() * 0.01:
                continue  # è·³è¿‡å æ¯”å°äº1%çš„ä¸šåŠ¡
            
            total_premium = business_df['matured_premium_yuan'].sum()
            total_claims = business_df['reported_claim_payment_yuan'].sum()
            loss_ratio = (total_claims / (total_premium + 1)) * 100
            
            # è®¡ç®—å½±å“åº¦
            org_premium = org_df['matured_premium_yuan'].sum()
            impact_score = loss_ratio * (total_premium / org_premium)
            
            business_analysis.append({
                'business_type': business_type,
                'loss_ratio': loss_ratio,
                'premium_ratio': (total_premium / org_premium) * 100,
                'impact_score': impact_score,
                'premium_amount': total_premium / 10000
            })
        
        return business_analysis
    
    def _analyze_by_coverage(self, business_df):
        """æŒ‰é™©åˆ«åˆ†æ"""
        coverage_analysis = []
        
        for coverage in business_df['coverage_type'].unique():
            if pd.isna(coverage):
                continue
                
            coverage_df = business_df[business_df['coverage_type'] == coverage]
            
            total_premium = coverage_df['matured_premium_yuan'].sum()
            total_claims = coverage_df['reported_claim_payment_yuan'].sum()
            loss_ratio = (total_claims / (total_premium + 1)) * 100
            
            coverage_analysis.append({
                'coverage_type': coverage,
                'loss_ratio': loss_ratio,
                'premium_ratio': (total_premium / business_df['matured_premium_yuan'].sum()) * 100
            })
        
        return coverage_analysis
    
    def _deep_anomaly_analysis(self, weekly_kpis):
        """æ·±åº¦å¼‚å¸¸åˆ†æ"""
        anomalies = []
        weeks = sorted(weekly_kpis.keys())
        
        # æ£€æŸ¥å•å‘¨æš´æ¶¨
        for i in range(1, len(weeks)):
            current_week = weeks[i]
            current = weekly_kpis[current_week]
            
            # æ¡ˆå‡èµ”æ¬¾æš´æ¶¨æ£€æµ‹
            prev_avg = weekly_kpis[weeks[i-1]]['æ¡ˆå‡èµ”æ¬¾']
            curr_avg = current['æ¡ˆå‡èµ”æ¬¾']
            
            if prev_avg > 0 and curr_avg > prev_avg * 1.5:
                anomalies.append({
                    'type': 'æ¡ˆå‡èµ”æ¬¾å•å‘¨æš´æ¶¨',
                    'week': current_week,
                    'severity': 'high' if curr_avg > prev_avg * 2 else 'medium',
                    'current_value': curr_avg,
                    'previous_value': prev_avg,
                    'change_ratio': round((curr_avg - prev_avg) / prev_avg, 2),
                    'possible_causes': ['å¤§é¢æ¡ˆä»¶é›†ä¸­', 'å®šæŸæ ‡å‡†æ”¾æ¾', 'æ¬ºè¯ˆæ¡ˆä»¶']
                })
        
        # æ£€æŸ¥è¿ç»­æ¶åŒ–
        if len(weeks) >= 4:
            loss_ratios = [weekly_kpis[w]['æ»¡æœŸèµ”ä»˜ç‡'] for w in weeks]
            
            # æ£€æŸ¥æœ€å3å‘¨æ˜¯å¦è¿ç»­ä¸Šå‡
            if all(loss_ratios[i] < loss_ratios[i+1] for i in range(-3, -1)):
                anomalies.append({
                    'type': 'èµ”ä»˜ç‡è¿ç»­æ¶åŒ–',
                    'weeks_affected': weeks[-3:],
                    'severity': 'high',
                    'trend': 'è¿ç»­3å‘¨ä¸Šå‡',
                    'change_magnitude': round(loss_ratios[-1] - loss_ratios[-3], 2)
                })
        
        return anomalies
    
    def _generate_mckinsey_recommendations(self, trend_insights):
        """ç”Ÿæˆéº¦è‚¯é”¡çº§å»ºè®®"""
        recommendations = {
            'immediate_actions': [],    # 24å°æ—¶å†…
            'short_term': [],           # 1å‘¨å†…
            'medium_term': [],          # 1ä¸ªæœˆå†…
            'strategic': []             # 3ä¸ªæœˆå†…
        }
        
        # åŸºäºè¶‹åŠ¿æ´å¯Ÿç”Ÿæˆå»ºè®®
        loss_trend = trend_insights.get('loss_ratio_trend', {})
        
        if loss_trend.get('direction') == 'ä¸Šå‡':
            recommendations['immediate_actions'].extend([
                "å¯åŠ¨é«˜é£é™©ä¸šåŠ¡äººå·¥å®¡æ ¸æœºåˆ¶",
                "æš‚åœé—®é¢˜ç»„åˆæ–°å•è‡ªåŠ¨æ ¸ä¿",
                "è°ƒå–æœ€è¿‘ä¸€å‘¨å¤§é¢æ¡ˆä»¶æ¸…å•"
            ])
            
            recommendations['short_term'].extend([
                "å¬å¼€é—®é¢˜æœºæ„ç´§æ€¥å¤ç›˜ä¼šè®®",
                "é‡æ–°è¯„ä¼°é«˜é£é™©ä¸šåŠ¡è´¹ç‡å……è¶³æ€§",
                "åŠ å¼ºç†èµ”ç¯èŠ‚åæ¬ºè¯ˆæ£€æŸ¥"
            ])
        
        # å¼‚å¸¸æ£€æµ‹å»ºè®®
        anomalies = trend_insights.get('anomalies', [])
        for anomaly in anomalies:
            if anomaly['type'] == 'æ¡ˆå‡èµ”æ¬¾å•å‘¨æš´æ¶¨':
                recommendations['immediate_actions'].append(
                    f"æ’æŸ¥ç¬¬{anomaly['week']}å‘¨æ¡ˆå‡çªå¢åŸå› ï¼Œé‡ç‚¹æ£€æŸ¥5ä¸‡å…ƒä»¥ä¸Šæ¡ˆä»¶"
                )
        
        return recommendations
    
    def _formulate_core_conclusion(self, latest_kpi, trend_insights):
        """å½¢æˆæ ¸å¿ƒç»“è®º"""
        loss_ratio = latest_kpi['æ»¡æœŸèµ”ä»˜ç‡']
        status = latest_kpi['èµ”ä»˜ç‡çŠ¶æ€']
        
        if status in ['å±é™©', 'é¢„è­¦']:
            trend_direction = trend_insights.get('loss_ratio_trend', {}).get('direction', 'æœªçŸ¥')
            
            if trend_direction == 'ä¸Šå‡':
                return f"ä¸šåŠ¡è¿›å…¥é«˜å±çŠ¶æ€ï¼Œèµ”ä»˜ç‡{loss_ratio}%ä¸”æŒç»­æ¶åŒ–ï¼Œéœ€ç«‹å³å¹²é¢„"
            else:
                return f"ä¸šåŠ¡å¤„äºé«˜å±çŠ¶æ€ï¼Œèµ”ä»˜ç‡{loss_ratio}%ï¼Œä½†è¶‹åŠ¿æš‚æ—¶ç¨³å®š"
        else:
            return f"ä¸šåŠ¡çŠ¶æ€{status}ï¼Œèµ”ä»˜ç‡{loss_ratio}%ï¼Œåœ¨å¯æ§èŒƒå›´å†…"
    
    def _determine_immediate_action(self, latest_kpi, trend_insights):
        """ç¡®å®šç«‹å³è¡ŒåŠ¨"""
        status = latest_kpi['èµ”ä»˜ç‡çŠ¶æ€']
        
        if status == 'å±é™©':
            return "ç«‹å³æš‚åœé«˜é£é™©æ–°å•æ‰¿ä¿ï¼Œå¯åŠ¨ç´§æ€¥é£æ§æªæ–½"
        elif status == 'é¢„è­¦':
            return "åŠ å¼ºé«˜é£é™©ä¸šåŠ¡å®¡æ ¸ï¼Œå¯†åˆ‡ç›‘æ§è¶‹åŠ¿å˜åŒ–"
        else:
            return "ç»´æŒç°æœ‰ç­–ç•¥ï¼ŒæŒç»­ç›‘æ§å…³é”®æŒ‡æ ‡"

# =======================================
# V2.0 æ–°èƒ½æºè´§è½¦åˆ†æå™¨
# =======================================

class NewEnergyTruckAnalyzer:
    """æ–°èƒ½æºè´§è½¦ä¸“é¡¹åˆ†æå™¨"""
    
    def analyze_new_energy_trucks(self, data_by_year):
        """åˆ†ææ–°èƒ½æºè´§è½¦ä¸šåŠ¡"""
        truck_analysis = {}
        
        for year in data_by_year.keys():
            if 'cumulative' not in data_by_year[year]:
                continue
                
            # è·å–æœ€æ–°æ•°æ®
            latest_week = max(data_by_year[year]['cumulative'].keys())
            latest_data = data_by_year[year]['cumulative'][latest_week]
            
            # ç­›é€‰æ–°èƒ½æºè´§è½¦
            truck_df = latest_data[
                (latest_data['is_new_energy_vehicle'] == True) & 
                (latest_data['business_type_category'].str.contains('è´§è½¦', na=False))
            ].copy()
            
            if len(truck_df) == 0:
                truck_analysis[year] = {"no_data": True}
                continue
            
            truck_analysis[year] = {
                'market_overview': self._analyze_market_overview(truck_df),
                'battery_risk_analysis': self._analyze_battery_risks(truck_df),
                'fleet_customer_analysis': self._analyze_fleet_customers(truck_df),
                'regional_risk_analysis': self._analyze_regional_risks(truck_df),
                'strategic_insights': self._generate_truck_insights(truck_df)
            }
        
        return truck_analysis
    
    def _analyze_market_overview(self, truck_df):
        """å¸‚åœºæ¦‚è§ˆåˆ†æ"""
        total_premium = truck_df['matured_premium_yuan'].sum()
        total_claims = truck_df['reported_claim_payment_yuan'].sum()
        loss_ratio = (total_claims / (total_premium + 1)) * 100
        
        # ä¸ä¼ ç»Ÿè´§è½¦å¯¹æ¯”
        traditional_trucks = truck_df[truck_df['is_new_energy_vehicle'] == False]
        traditional_loss_ratio = (traditional_trucks['reported_claim_payment_yuan'].sum() / 
                                (traditional_trucks['matured_premium_yuan'].sum() + 1)) * 100
        
        return {
            'premium_scale': total_premium / 10000,
            'loss_ratio': loss_ratio,
            'policy_count': len(truck_df),
            'claim_count': truck_df['claim_case_count'].sum(),
            'comparison': {
                'traditional_truck_loss_ratio': traditional_loss_ratio,
                'loss_ratio_premium': loss_ratio - traditional_loss_ratio
            }
        }
    
    def _analyze_battery_risks(self, truck_df):
        """ç”µæ± é£é™©åˆ†æ"""
        # ç®€åŒ–ç‰ˆç”µæ± ç›¸å…³ç†èµ”è¯†åˆ«
        battery_claims = truck_df[truck_df['reported_claim_payment_yuan'] > 50000]  # å‡è®¾å¤§é¢ç†èµ”ä¸ç”µæ± ç›¸å…³
        
        total_premium = truck_df['matured_premium_yuan'].sum()
        battery_claims_amount = battery_claims['reported_claim_payment_yuan'].sum()
        battery_claim_ratio = (battery_claims_amount / total_premium) * 100
        
        avg_battery_claim = battery_claims['reported_claim_payment_yuan'].mean() if len(battery_claims) > 0 else 0
        
        return {
            'battery_claim_ratio': battery_claim_ratio,
            'avg_battery_claim': avg_battery_claim,
            'battery_claim_count': len(battery_claims),
            'risk_level': "é«˜" if battery_claim_ratio > 25 else "ä¸­" if battery_claim_ratio > 15 else "ä½"
        }
    
    def _analyze_fleet_customers(self, truck_df):
        """è½¦é˜Ÿå®¢æˆ·åˆ†æ"""
        # æŒ‰æœºæ„åˆ†ç»„æ¨¡æ‹Ÿä¸åŒè§„æ¨¡è½¦é˜Ÿ
        fleet_analysis = []
        
        for org in truck_df['third_level_organization'].unique():
            org_df = truck_df[truck_df['third_level_organization'] == org]
            
            if len(org_df) < 5:  # å°å®¢æˆ·
                fleet_type = "æ•£æˆ·"
            elif len(org_df) < 20:  # ä¸­å‹å®¢æˆ·
                fleet_type = "ä¸­å°è½¦é˜Ÿ"
            else:  # å¤§å‹å®¢æˆ·
                fleet_type = "å¤§å‹è½¦é˜Ÿ"
            
            total_premium = org_df['matured_premium_yuan'].sum()
            total_claims = org_df['reported_claim_payment_yuan'].sum()
            loss_ratio = (total_claims / (total_premium + 1)) * 100
            
            fleet_analysis.append({
                'organization': org,
                'fleet_type': fleet_type,
                'vehicle_count': len(org_df),
                'loss_ratio': loss_ratio,
                'premium_amount': total_premium / 10000
            })
        
        return fleet_analysis
    
    def _analyze_regional_risks(self, truck_df):
        """åŒºåŸŸé£é™©åˆ†æ"""
        regional_analysis = []
        
        # æ¨¡æ‹Ÿä¸åŒåŸå¸‚ç­‰çº§
        city_tiers = {
            'æˆéƒ½': 'ä¸€çº¿åŸå¸‚',
            'é«˜æ–°': 'äºŒçº¿åŸå¸‚', 
            'å¤©åºœ': 'äºŒçº¿åŸå¸‚',
            'åŒæµ': 'ä¸‰çº¿åŸå¸‚'
        }
        
        for org in truck_df['third_level_organization'].unique():
            org_df = truck_df[truck_df['third_level_organization'] == org]
            
            city_tier = city_tiers.get(org, 'ä¸‰çº¿åŸå¸‚')
            
            total_premium = org_df['matured_premium_yuan'].sum()
            total_claims = org_df['reported_claim_payment_yuan'].sum()
            loss_ratio = (total_claims / (total_premium + 1)) * 100
            
            # æ¨¡æ‹ŸåŸºç¡€è®¾æ–½å¯†åº¦å½±å“
            infrastructure_factor = {
                'ä¸€çº¿åŸå¸‚': 1.0,
                'äºŒçº¿åŸå¸‚': 1.3,
                'ä¸‰çº¿åŸå¸‚': 1.8
            }.get(city_tier, 1.5)
            
            adjusted_loss_ratio = loss_ratio * infrastructure_factor
            
            regional_analysis.append({
                'organization': org,
                'city_tier': city_tier,
                'actual_loss_ratio': loss_ratio,
                'infrastructure_adjusted_ratio': adjusted_loss_ratio,
                'risk_level': "é«˜" if adjusted_loss_ratio > 80 else "ä¸­" if adjusted_loss_ratio > 60 else "ä½"
            })
        
        return regional_analysis
    
    def _generate_truck_insights(self, truck_df):
        """ç”Ÿæˆæ–°èƒ½æºè´§è½¦æ´å¯Ÿ"""
        return {
            'key_findings': [
                "æ–°èƒ½æºè´§è½¦èµ”ä»˜ç‡æ™®éé«˜äºä¼ ç»Ÿè´§è½¦15-25ä¸ªç™¾åˆ†ç‚¹",
                "åŸºç¡€è®¾æ–½ä¸å®Œå–„åœ°åŒºé£é™©æ˜¾è‘—å¢é«˜",
                "å¤§å‹è½¦é˜Ÿå®¢æˆ·ç®¡ç†ç›¸å¯¹è§„èŒƒï¼Œä½†é«˜é¢‘ä½¿ç”¨åŠ é€Ÿé£é™©æš´éœ²"
            ],
            'immediate_actions': [
                "æš‚åœä¸‰çº¿åŸå¸‚æ–°èƒ½æºè´§è½¦æ–°å•æ‰¿ä¿",
                "å¯¹å¤§å‹è½¦é˜Ÿå®¢æˆ·è¦æ±‚BMSæ•°æ®æ¥å…¥",
                "å»ºç«‹ç”µæ± å¥åº·åº¦è·Ÿè¸ªæœºåˆ¶"
            ]
        }

# =======================================
# V2.0 æŠ¥å‘Šç”Ÿæˆå™¨ (éº¦è‚¯é”¡çº§)
# =======================================

class McKinseyReportGenerator:
    """éº¦è‚¯é”¡çº§æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def generate_comprehensive_report(self, trend_report, new_energy_analysis, global_kpis):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        
        reports = {}
        
        for year in trend_report.keys():
            if 'executive_summary' not in trend_report[year]:
                continue
                
            # ç”Ÿæˆå¹´åº¦ç»¼åˆæŠ¥å‘Š
            yearly_report = self._generate_yearly_report(
                year, trend_report[year], new_energy_analysis.get(year), global_kpis
            )
            
            reports[f"{year}å¹´åº¦"] = yearly_report
        
        return reports
    
    def _generate_yearly_report(self, year, trend_data, new_energy_data, global_kpis):
        """ç”Ÿæˆå¹´åº¦æŠ¥å‘Š"""
        
        # æŠ¥å‘Šç»“æ„
        report_sections = [
            self._generate_executive_summary(year, trend_data, new_energy_data),
            self._generate_problem_analysis(year, trend_data),
            self._generate_new_energy_section(year, new_energy_data),
            self._generate_strategic_recommendations(year, trend_data),
            self._generate_implementation_roadmap(year, trend_data)
        ]
        
        return "\n\n".join(report_sections)
    
    def _generate_executive_summary(self, year, trend_data, new_energy_data):
        """æ‰§è¡Œæ‘˜è¦ (é‡‘å­—å¡”åŸç†)"""
        
        summary_data = trend_data['executive_summary']
        
        if summary_data.get('insufficient_data'):
            return f"# {year}å¹´åº¦è½¦é™©ä¸šåŠ¡è¶‹åŠ¿è¿½è¸ªæŠ¥å‘Š\n\n## æ‰§è¡Œæ‘˜è¦\n\næ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æã€‚"
        
        core_conclusion = summary_data['core_conclusion']
        key_supports = summary_data['key_supports']
        immediate_action = summary_data['immediate_action']
        
        # æ„å»ºé‡‘å­—å¡”ç»“æ„
        summary = f"""# {year}å¹´åº¦è½¦é™©ä¸šåŠ¡è¶‹åŠ¿è¿½è¸ªæŠ¥å‘Š

## æ‰§è¡Œæ‘˜è¦

### æ ¸å¿ƒç»“è®º
{core_conclusion}

### å…³é”®æ”¯æ’‘
"""
        
        for i, support in enumerate(key_supports, 1):
            summary += f"{i}. {support}\n"
        
        summary += f"""
### ç«‹å³è¡ŒåŠ¨å»ºè®®
{immediate_action}

---
"""
        
        return summary
    
    def _generate_problem_analysis(self, year, trend_data):
        """é—®é¢˜åˆ†æéƒ¨åˆ†"""
        
        problem_orgs = trend_data.get('problem_organizations', [])
        
        analysis = f"""## ç¬¬ä¸€éƒ¨åˆ†ï¼šé—®é¢˜æœºæ„æ·±åº¦è¯Šæ–­

### é£é™©æœºæ„æ’å (TOP3)
"""
        
        for i, org_info in enumerate(problem_orgs[:3], 1):
            org_name = org_info['organization']
            risk_score = org_info['risk_score']
            kpis = org_info['kpis']
            
            analysis += f"""
#### {i}. {org_name} (é£é™©è¯„åˆ†: {risk_score}/100)

**é£é™©æ€§è´¨**: ğŸ”´ ä¸¥é‡å¤±æ§ - èµ”ä»˜ç‡{kpis['loss_ratio']:.1f}%ï¼Œè¶…è­¦æˆ’çº¿{kpis['loss_ratio']-80:.1f}ä¸ªç™¾åˆ†ç‚¹

**æ ¸å¿ƒæŒ‡æ ‡**:
- æ»¡æœŸä¿è´¹: {kpis['premium_scale']:.2f}ä¸‡å…ƒ
- èµ”ä»˜ç‡: {kpis['loss_ratio']:.1f}%
- èµ”æ¡ˆä»¶æ•°: {kpis['claim_cases']}ä»¶
- ä¿å•ä»¶æ•°: {kpis['policies']}ä»¶

**é—®é¢˜ä¸šåŠ¡ç±»å‹**:
"""
            
            for j, business_info in enumerate(org_info.get('problem_businesses', [])[:3], 1):
                analysis += f"""
{j}. **{business_info['business_type']}**
   - èµ”ä»˜ç‡: {business_info['loss_ratio']:.1f}%
   - å æœºæ„ä¿è´¹: {business_info['premium_ratio']:.1f}%
   - å½±å“åº¦è¯„åˆ†: {business_info['impact_score']:.1f}
"""
                
                worst_coverage = business_info.get('worst_coverage')
                if worst_coverage:
                    analysis += f"   - æœ€å·®é™©åˆ«: {worst_coverage['coverage_type']} ({worst_coverage['loss_ratio']:.1f}%èµ”ä»˜ç‡)\n"
        
        return analysis
    
    def _generate_new_energy_section(self, year, new_energy_data):
        """æ–°èƒ½æºè´§è½¦ä¸“é¡¹åˆ†æ"""
        
        if not new_energy_data or new_energy_data.get('no_data'):
            return f"""
## ç¬¬äºŒéƒ¨åˆ†ï¼šæ–°èƒ½æºè´§è½¦ä¸“é¡¹åˆ†æ

{year}å¹´åº¦æ–°èƒ½æºè´§è½¦ä¸šåŠ¡æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œä¸“é¡¹åˆ†æã€‚
"""
        
        market_overview = new_energy_data.get('market_overview', {})
        battery_risk = new_energy_data.get('battery_risk_analysis', {})
        regional_analysis = new_energy_data.get('regional_risk_analysis', [])
        insights = new_energy_data.get('strategic_insights', {})
        
        section = f"""
## ç¬¬äºŒéƒ¨åˆ†ï¼šæ–°èƒ½æºè´§è½¦ä¸“é¡¹åˆ†æ

### å¸‚åœºæ¦‚è§ˆ
æ–°èƒ½æºè´§è½¦ä¸šåŠ¡å‘ˆç°"ä¸‰é«˜"ç‰¹å¾ï¼š
- ä¸šåŠ¡è§„æ¨¡: {market_overview.get('premium_scale', 0):.2f}ä¸‡å…ƒ
- èµ”ä»˜ç‡: {market_overview.get('loss_ratio', 0):.1f}% (vs ä¼ ç»Ÿè´§è½¦{market_overview.get('comparison', {}).get('traditional_truck_loss_ratio', 0):.1f}%)
- ä¿å•ä»¶æ•°: {market_overview.get('policy_count', 0)}ä»¶

### ç”µæ± é£é™©åˆ†æ
- ç”µæ± ç›¸å…³ç†èµ”å æ¯”: {battery_risk.get('battery_claim_ratio', 0):.1f}%
- ç”µæ± æ¡ˆå‡èµ”æ¬¾: {battery_risk.get('avg_battery_claim', 0):,.0f}å…ƒ
- é£é™©ç­‰çº§: {battery_risk.get('risk_level', 'æœªçŸ¥')}

### åŒºåŸŸé£é™©åˆ†å¸ƒ
"""
        
        for region_info in regional_analysis[:3]:
            section += f"- {region_info['organization']}: {region_info['city_tier']} ({region_info['risk_level']}é£é™©)\n"
        
        section += f"""
### æ ¸å¿ƒæ´å¯Ÿ
"""
        
        for finding in insights.get('key_findings', []):
            section += f"- {finding}\n"
        
        return section
    
    def _generate_strategic_recommendations(self, year, trend_data):
        """æˆ˜ç•¥å»ºè®® (So Whatæ€ç»´)"""
        
        recommendations = trend_data.get('strategic_recommendations', {})
        anomalies = trend_data.get('anomaly_analysis', [])
        
        section = f"""
## ç¬¬ä¸‰éƒ¨åˆ†ï¼šæˆ˜ç•¥å»ºè®®ä¸è¡ŒåŠ¨è®¡åˆ’

### ğŸš¨ ç«‹å³è¡ŒåŠ¨ (24å°æ—¶å†…)
"""
        
        for action in recommendations.get('immediate_actions', []):
            section += f"- {action}\n"
        
        section += f"""
### â° æœ¬å‘¨å†…å®Œæˆ (7å¤©)
"""
        
        for action in recommendations.get('short_term', []):
            section += f"- {action}\n"
        
        section += f"""
### ğŸ“Š ä¸­æœŸä¼˜åŒ– (1ä¸ªæœˆå†…)
"""
        
        for action in recommendations.get('medium_term', []):
            section += f"- {action}\n"
        
        # å¼‚å¸¸å¤„ç†å»ºè®®
        if anomalies:
            section += f"""
### âš ï¸ å¼‚å¸¸äº‹ä»¶å¤„ç†
"""
            for anomaly in anomalies[:3]:
                section += f"- {anomaly['type']}: {anomaly.get('recommended_action', 'éœ€ä¸“é¡¹è°ƒæŸ¥')}\n"
        
        return section
    
    def _generate_implementation_roadmap(self, year, trend_data):
        """å®æ–½è·¯çº¿å›¾"""
        
        return f"""
---

## å®æ–½ä¿éšœæœºåˆ¶

### æ—¶é—´è¡¨ç›‘æ§
- æ¯æ—¥: å…³é”®æŒ‡æ ‡è·Ÿè¸ª
- æ¯å‘¨: è¶‹åŠ¿åˆ†ææŠ¥å‘Š
- æ¯æœˆ: æ•ˆæœè¯„ä¼°æ€»ç»“

### è´£ä»»åˆ†å·¥
- è½¦é™©éƒ¨: ä¸šåŠ¡ç­–ç•¥æ‰§è¡Œ
- ç†èµ”éƒ¨: å®šæŸæ ‡å‡†ç®¡æ§  
- é£æ§éƒ¨: é£é™©ç›‘æµ‹é¢„è­¦
- è´¢åŠ¡éƒ¨: æ•ˆæœé‡åŒ–è¯„ä¼°

### æˆåŠŸæ ‡å‡†
- çŸ­æœŸç›®æ ‡: é—®é¢˜æœºæ„èµ”ä»˜ç‡ä¸‹é™5ä¸ªç™¾åˆ†ç‚¹
- ä¸­æœŸç›®æ ‡: æ–°èƒ½æºè´§è½¦é£é™©å¯æ§
- é•¿æœŸç›®æ ‡: æ•´ä½“ä¸šåŠ¡è´¨é‡æ˜¾è‘—æå‡

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*æ•°æ®æ¥æº: {year}å¹´åº¦ä¿å•ç´¯è®¡æ•°æ®*
*åˆ†æå‘¨æœŸ: æœ€è¿‘{LOOKBACK_WEEKS}å‘¨è¶‹åŠ¿*
"""

# =======================================
# ä¸»å‡½æ•°
# =======================================

def main():
    """ä¸»å‡½æ•° - V2.0å®Œæ•´æµç¨‹"""
    
    print("ğŸš€ å¯åŠ¨è½¦é™©å‘¨æŠ¥ç”Ÿæˆå™¨ V2.0 - éº¦è‚¯é”¡çº§è¶‹åŠ¿è¿½è¸ª")
    print("=" * 60)
    
    try:
        # Step 1: V2.0æ•°æ®åŠ è½½
        print("\nğŸ“Š Step 1: V2.0æ•°æ®åŠ è½½...")
        loader = InsuranceDataLoaderV2()
        
        # ç¡®å®šåˆ†æå‘¨æœŸ
        period_info = loader.determine_analysis_period(START_WEEK, END_WEEK, LOOKBACK_WEEKS)
        print(f"åˆ†æå‘¨æœŸ: ç¬¬{period_info['start_week']}-{period_info['end_week']}å‘¨")
        print(f"å›æº¯å‘¨æ•°: {period_info['lookback_weeks']}å‘¨")
        print(f"ç¼ºå¤±å‘¨æ¬¡: {period_info['missing_weeks']}")
        
        # åŠ è½½æ•°æ®
        loaded_data, load_errors = loader.load_data_files(period_info['weeks_to_load'])
        if not loaded_data:
            raise RuntimeError("æœªæˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
        
        # é¢„å¤„ç†
        loaded_data = loader.preprocess_data(loaded_data)
        
        # è®¡ç®—å½“å‘¨å€¼
        weekly_data = loader.calculate_weekly_values(loaded_data, period_info['analysis_weeks'])
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œæ¶‰åŠ{len(weekly_data)}ä¸ªä¿å•å¹´åº¦")
        
        # Step 2: V2.0 KPIè®¡ç®—
        print("\nğŸ“ˆ Step 2: V2.0 KPIè®¡ç®—...")
        calculator = InsuranceKpiCalculatorV2()
        
        # è®¡ç®—å…¨å±€KPI
        global_kpis = {}
        for year in weekly_data.keys():
            if 'cumulative' in weekly_data[year] and weekly_data[year]['cumulative']:
                latest_week = max(weekly_data[year]['cumulative'].keys())
                latest_data = weekly_data[year]['cumulative'][latest_week]
                global_kpis[year] = calculator.calculate_kpis(latest_data, 'cumulative')
        
        # è®¡ç®—è¶‹åŠ¿KPI
        trend_kpis = calculator.calculate_trend_kpis(weekly_data)
        print("âœ… KPIè®¡ç®—å®Œæˆ")
        
        # Step 3: V2.0è¶‹åŠ¿è¿½è¸ª (å¦‚å¯ç”¨)
        trend_report = {}
        if ENABLE_TREND_TRACKING:
            print("\nğŸ“ˆ Step 3: V2.0è¶‹åŠ¿è¿½è¸ªåˆ†æ...")
            trend_tracker = InsuranceLossTrendTrackerV2()
            trend_report = trend_tracker.analyze_trends(weekly_data, trend_kpis)
            print("âœ… è¶‹åŠ¿åˆ†æå®Œæˆ")
        
        # Step 4: æ–°èƒ½æºè´§è½¦åˆ†æ (å¦‚å¯ç”¨)
        new_energy_analysis = {}
        if ENABLE_NEW_ENERGY_TRUCK:
            print("\nğŸš› Step 4: æ–°èƒ½æºè´§è½¦ä¸“é¡¹åˆ†æ...")
            truck_analyzer = NewEnergyTruckAnalyzer()
            new_energy_analysis = truck_analyzer.analyze_new_energy_trucks(weekly_data)
            print("âœ… æ–°èƒ½æºè´§è½¦åˆ†æå®Œæˆ")
        
        # Step 5: éº¦è‚¯é”¡çº§æŠ¥å‘Šç”Ÿæˆ
        print("\nğŸ“‹ Step 5: ç”Ÿæˆéº¦è‚¯é”¡çº§æŠ¥å‘Š...")
        report_generator = McKinseyReportGenerator()
        final_reports = report_generator.generate_comprehensive_report(
            trend_report, new_energy_analysis, global_kpis
        )
        
        # ä¿å­˜æŠ¥å‘Š
        output_path = Path(OUTPUT_FOLDER)
        output_path.mkdir(exist_ok=True)
        
        for year_label, report_content in final_reports.items():
            # æå–å¹´ä»½æ•°å­—
            year_match = re.search(r'(2024|2025)', year_label)
            year_num = year_match.group(1) if year_match else "unknown"
            
            filename = f"{year_num}ä¿å•è¶‹åŠ¿è¿½è¸ªæŠ¥å‘Š_ç¬¬{period_info['end_week']}å‘¨.md"
            filepath = output_path / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {filename} ({len(report_content):,}å­—ç¬¦)")
        
        # è¾“å‡ºæ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ‰ V2.0æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        print("=" * 60)
        
        for year in global_kpis.keys():
            if year in global_kpis:
                kpis = global_kpis[year]
                print(f"\n{year}ä¿å•æ ¸å¿ƒæŒ‡æ ‡:")
                print(f"  - æ»¡æœŸä¿è´¹: {kpis['æ»¡æœŸä¿è´¹']:.2f}ä¸‡å…ƒ")
                print(f"  - èµ”ä»˜ç‡: {kpis['æ»¡æœŸèµ”ä»˜ç‡']:.1f}% ({kpis['èµ”ä»˜ç‡çŠ¶æ€']})")
                print(f"  - è¾¹è´¡ç‡: {kpis['æ»¡æœŸè¾¹é™…è´¡çŒ®ç‡']:.1f}% ({kpis['è¾¹è´¡ç‡çŠ¶æ€']})")
        
        print(f"\nğŸ“ æŠ¥å‘Šä½ç½®: {output_path.absolute()}")
        print(f"ğŸ“Š åˆ†æå‘¨æœŸ: ç¬¬{period_info['start_week']}-{period_info['end_week']}å‘¨")
        print(f"ğŸ” åŠŸèƒ½å¯ç”¨: è¶‹åŠ¿è¿½è¸ª{'âœ…' if ENABLE_TREND_TRACKING else 'âŒ'}, "
              f"æ–°èƒ½æºåˆ†æ{'âœ…' if ENABLE_NEW_ENERGY_TRUCK else 'âŒ'}, "
              f"éº¦è‚¯é”¡æ¡†æ¶{'âœ…' if ENABLE_MCKINSEY_FRAMEWORK else 'âŒ'}")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)