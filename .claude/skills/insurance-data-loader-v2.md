---
name: insurance-data-loader-v2
description: è½¦é™©æ•°æ®åŠ è½½å™¨V2.0 - æ”¯æŒæ™ºèƒ½å‘¨æœŸç®¡ç†ã€å½“å‘¨å‘ç”Ÿå€¼è®¡ç®—ã€å¤šå‘¨è¶‹åŠ¿åˆ†æï¼Œä¸ºV2.0è¶‹åŠ¿è¿½è¸ªæä¾›æ•°æ®åŸºç¡€ï¼Œæ”¯æŒæ‰‹åŠ¨å‘¨æœŸè¦†ç›–å’Œè‡ªåŠ¨æ•°æ®å®Œæ•´æ€§éªŒè¯
---

# insurance-data-loader-v2

## æ¦‚è¿°

**V2.0å‡çº§æ ¸å¿ƒèƒ½åŠ›**:
- âœ… **æ™ºèƒ½å‘¨æœŸç®¡ç†**: è‡ªåŠ¨æ¨æ–­æœ€ä½³åˆ†æå‘¨æœŸ + æ‰‹åŠ¨çµæ´»è¦†ç›–
- âœ… **å½“å‘¨å‘ç”Ÿå€¼è®¡ç®—**: æ”¯æŒç´¯è®¡æ•°æ®å·®å€¼æ³•ï¼Œè‡ªåŠ¨åŠ è½½å‰ä¸€å‘¨æ•°æ®
- âœ… **å¤šç‰ˆæœ¬æ•°æ®å…¼å®¹**: æ”¯æŒ2024/2025ä¿å•å¹´åº¦æ··åˆåˆ†æ
- âœ… **æ•°æ®è´¨é‡ç›‘æ§**: å®Œæ•´æ€§éªŒè¯ã€å¼‚å¸¸å€¼æ£€æµ‹ã€ç¼ºå¤±å€¼å¤„ç†
- âœ… **V2.0è¶‹åŠ¿è¿½è¸ª**: ä¸“ä¸ºinsurance-loss-trend-trackerä¼˜åŒ–

**å…³é”®æ”¹è¿›**: 
- è‡ªåŠ¨æ‰«æå¯ç”¨æ•°æ®æ–‡ä»¶ï¼Œæ™ºèƒ½æ¨èåˆ†æå‘¨æœŸ
- è‡ªåŠ¨åŠ è½½å‰ä¸€å‘¨æ•°æ®ç”¨äºå½“å‘¨å€¼è®¡ç®—
- æ”¯æŒæŒ‡å®šèµ·æ­¢å‘¨æœŸçš„çµæ´»åˆ†æ
- å¢å¼ºé”™è¯¯å¤„ç†å’Œå®¹é”™æœºåˆ¶

## æ ¸å¿ƒæ¦‚å¿µ

### 1. æ™ºèƒ½å‘¨æœŸç®¡ç†

**è‡ªåŠ¨æ¨æ–­é€»è¾‘**:
```
1. æ‰«ææ•°æ®ç›®å½•ï¼Œè¯†åˆ«æ‰€æœ‰å¯ç”¨å‘¨æ¬¡
2. æ‰¾åˆ°æœ€æ–°å¯ç”¨å‘¨æ¬¡ä½œä¸ºEND_WEEK
3. è‡ªåŠ¨å›æº¯LOOKBACK_WEEKS(é»˜è®¤5å‘¨)ä½œä¸ºSTART_WEEK
4. è‡ªåŠ¨åŠ è½½å‰ä¸€å‘¨(START_WEEK-1)ç”¨äºå½“å‘¨å€¼è®¡ç®—
```

**æ‰‹åŠ¨è¦†ç›–æœºåˆ¶**:
```python
# ç”¨æˆ·å¯åœ¨è„šæœ¬å¼€å¤´æŒ‡å®š
START_WEEK = 35  # æ‰‹åŠ¨æŒ‡å®šèµ·å§‹å‘¨
END_WEEK = 39    # æ‰‹åŠ¨æŒ‡å®šç»“æŸå‘¨
LOOKBACK_WEEKS = 7  # æ‰‹åŠ¨æŒ‡å®šå›æº¯å‘¨æ•°
```

### 2. å½“å‘¨å‘ç”Ÿå€¼è®¡ç®—åŸç†

**ç´¯è®¡æ•°æ®ç‰¹å¾**: CSVæ–‡ä»¶å­˜å‚¨çš„æ˜¯**å¹´åº¦ç´¯è®¡æ•°æ®**

**å½“å‘¨å€¼è®¡ç®—å…¬å¼**:
```
å½“å‘¨å‘ç”Ÿå€¼ = æœ¬å‘¨ç´¯è®¡å€¼ - ä¸Šå‘¨ç´¯è®¡å€¼
```

**éœ€è¦å½“å‘¨å€¼çš„æŒ‡æ ‡**:
- âœ… å·²æŠ¥å‘Šèµ”æ¬¾ (å½“å‘¨æ–°å¢èµ”æ¬¾)
- âœ… æ»¡æœŸä¿è´¹ (å½“å‘¨æ»¡æœŸä¿è´¹)  
- âœ… ç­¾å•ä¿è´¹ (å½“å‘¨æ–°ç­¾ä¿è´¹)
- âœ… èµ”æ¡ˆä»¶æ•° (å½“å‘¨æ–°å¢æ¡ˆä»¶)
- âŒ èµ”ä»˜ç‡ (åªç”¨ç´¯è®¡å€¼è®¡ç®—ï¼Œä¿æŒç¨³å®š)

### 3. å‰ä¸€å‘¨ç¼ºå¤±å¤„ç†

**å®¹é”™æœºåˆ¶**:
- å¦‚æœå‰ä¸€å‘¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå½“å‘¨å€¼æ ‡è®°ä¸º"N/A"
- æŠ¥å‘Šç”Ÿæˆä¸å—å½±å“ï¼Œç»§ç»­æ‰§è¡Œ
- åœ¨æŠ¥å‘Šä¸­æ˜ç¡®æ ‡æ³¨ç¼ºå¤±æ•°æ®çš„å‘¨æ¬¡

## è¾“å…¥å‚æ•°

**V2.0æ–°å¢å‚æ•°**:
```python
# æ™ºèƒ½å‘¨æœŸç®¡ç†
start_week: int = None        # æ‰‹åŠ¨æŒ‡å®šèµ·å§‹å‘¨ï¼ŒNone=è‡ªåŠ¨æ¨æ–­
end_week: int = None          # æ‰‹åŠ¨æŒ‡å®šç»“æŸå‘¨ï¼ŒNone=è‡ªåŠ¨æ¨æ–­
lookback_weeks: int = 5       # å›æº¯å‘¨æ•°ï¼Œé»˜è®¤5å‘¨

# æ•°æ®è´¨é‡æ§åˆ¶
validate_data: bool = True    # æ˜¯å¦è¿›è¡Œæ•°æ®å®Œæ•´æ€§éªŒè¯
tolerance_missing: float = 0.2 # ç¼ºå¤±æ•°æ®å®¹å¿åº¦(20%ä»¥å†…ç»§ç»­æ‰§è¡Œ)

# V2.0ä¸“é¡¹é…ç½®
enable_weekly_calculation: bool = True  # å¯ç”¨å½“å‘¨å€¼è®¡ç®—
auto_detect_cycles: bool = True        # å¯ç”¨æ™ºèƒ½å‘¨æœŸæ£€æµ‹
```

## æ‰§è¡Œæ­¥éª¤ (V2.0å¢å¼º)

### Step 1: æ™ºèƒ½å‘¨æœŸæ£€æµ‹ (V2.0æ–°å¢)

```python
def detect_available_weeks(data_folder="å¤„ç†å/"):
    """æ‰«æç›®å½•ï¼Œæ£€æµ‹æ‰€æœ‰å¯ç”¨å‘¨æ¬¡"""
    from pathlib import Path
    import re
    
    data_path = Path(data_folder)
    available_weeks = set()
    
    # æ‰«ææ‰€æœ‰CSVæ–‡ä»¶
    for csv_file in data_path.glob("*ä¿å•ç¬¬*å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv"):
        # æå–å‘¨æ¬¡æ•°å­—
        match = re.search(r'ç¬¬(\d+)å‘¨', csv_file.name)
        if match:
            week_num = int(match.group(1))
            available_weeks.add(week_num)
    
    return sorted(available_weeks)

def determine_analysis_period(start_week, end_week, lookback_weeks):
    """ç¡®å®šåˆ†æå‘¨æœŸ"""
    available_weeks = detect_available_weeks()
    
    if not available_weeks:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•")
    
    # è‡ªåŠ¨æ¨æ–­æ¨¡å¼
    if start_week is None and end_week is None:
        end_week = max(available_weeks)
        start_week = end_week - lookback_weeks + 1
    
    # åŠè‡ªåŠ¨æ¨¡å¼
    elif start_week is None:
        start_week = end_week - lookback_weeks + 1
    elif end_week is None:
        end_week = start_week + lookback_weeks - 1
    
    # éªŒè¯å‘¨æœŸæœ‰æ•ˆæ€§
    analysis_weeks = list(range(start_week, end_week + 1))
    missing_weeks = [w for w in analysis_weeks if w not in available_weeks]
    
    if missing_weeks:
        missing_ratio = len(missing_weeks) / len(analysis_weeks)
        if missing_ratio > tolerance_missing:
            raise ValueError(f"ç¼ºå¤±æ•°æ®æ¯”ä¾‹{missing_ratio:.1%}è¶…è¿‡å®¹å¿åº¦{tolerance_missing:.1%}")
        else:
            print(f"âš ï¸  ç¼ºå¤±å‘¨æ¬¡{missing_weeks}ï¼Œä½†æ¯”ä¾‹{missing_ratio:.1%}åœ¨å®¹å¿èŒƒå›´å†…")
    
    # ç¡®å®šéœ€è¦åŠ è½½çš„å‘¨æ¬¡ï¼ˆåŒ…æ‹¬å‰ä¸€å‘¨ç”¨äºè®¡ç®—ï¼‰
    weeks_to_load = list(range(start_week - 1, end_week + 1))
    weeks_to_load = [w for w in weeks_to_load if w in available_weeks]
    
    return start_week, end_week, analysis_weeks, weeks_to_load, missing_weeks
```

### Step 2: å¢å¼ºæ•°æ®åŠ è½½ (V2.0æ”¹è¿›)

```python
def load_data_with_validation(weeks_to_load, data_folder="å¤„ç†å/"):
    """åŠ è½½æ•°æ®å¹¶éªŒè¯å®Œæ•´æ€§"""
    from pathlib import Path
    import pandas as pd
    
    data_path = Path(data_folder)
    loaded_data = {}
    load_errors = []
    
    # V2.0å¢å¼ºå­—æ®µéªŒè¯
    REQUIRED_FIELDS_V2 = REQUIRED_FIELDS.copy()
    REQUIRED_FIELDS_V2.extend(['week_number', 'snapshot_date'])  # V2.0å¿…éœ€å­—æ®µ
    
    for week in weeks_to_load:
        pattern = f"*ä¿å•ç¬¬{week}å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv"
        matching_files = list(data_path.glob(pattern))
        
        if not matching_files:
            load_errors.append(f"ç¬¬{week}å‘¨: æœªæ‰¾åˆ°æ–‡ä»¶")
            continue
            
        week_data = []
        for file in matching_files:
            try:
                df = pd.read_csv(file, encoding='utf-8-sig')
                
                # V2.0å¢å¼ºéªŒè¯
                missing_fields = set(REQUIRED_FIELDS_V2) - set(df.columns)
                if missing_fields:
                    print(f"âš ï¸  {file.name} ç¼ºå°‘å­—æ®µ: {missing_fields}")
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                if len(df) == 0:
                    print(f"âš ï¸  {file.name} æ•°æ®ä¸ºç©º")
                    continue
                    
                # æ·»åŠ å‘¨æ¬¡æ ‡è¯†
                df['week_number'] = week
                df['data_source'] = file.name
                
                week_data.append(df)
                
            except Exception as e:
                print(f"âŒ åŠ è½½ {file.name} å¤±è´¥: {e}")
                load_errors.append(f"ç¬¬{week}å‘¨: {str(e)}")
        
        if week_data:
            # åˆå¹¶åŒä¸€å‘¨çš„å¤šä¸ªæ–‡ä»¶
            combined_df = pd.concat(week_data, ignore_index=True)
            loaded_data[week] = combined_df
            print(f"âœ… ç¬¬{week}å‘¨: æˆåŠŸåŠ è½½ {len(combined_df)} è¡Œæ•°æ®")
        else:
            load_errors.append(f"ç¬¬{week}å‘¨: æ— æœ‰æ•ˆæ•°æ®")
    
    return loaded_data, load_errors
```

### Step 3: V2.0æ•°æ®é¢„å¤„ç† (æ–°å¢)

```python
def preprocess_v2_data(loaded_data):
    """V2.0æ•°æ®é¢„å¤„ç†"""
    
    for week, df in loaded_data.items():
        print(f"\nğŸ”§ é¢„å¤„ç†ç¬¬{week}å‘¨æ•°æ®...")
        
        # 1. è¿‡æ»¤æœ¬éƒ¨æœºæ„
        original_count = len(df)
        df_filtered = df[df['third_level_organization'] != 'æœ¬éƒ¨'].copy()
        filtered_count = len(df_filtered)
        
        print(f"  - è¿‡æ»¤æœ¬éƒ¨: {original_count} â†’ {filtered_count} ({original_count - filtered_count}è¡Œå‰”é™¤)")
        
        # 2. å¹´åº¦åˆ†ç»„
        df_filtered['policy_year'] = df_filtered['policy_start_year'].astype(str).str.extract(r'(202[45])')[0]
        
        # 3. æ•°æ®ç±»å‹æ ‡å‡†åŒ–
        numeric_columns = [
            'signed_premium_yuan', 'matured_premium_yuan', 'reported_claim_payment_yuan',
            'expense_amount_yuan', 'marginal_contribution_amount_yuan'
        ]
        
        for col in numeric_columns:
            df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce').fillna(0)
        
        # 4. å¼‚å¸¸å€¼æ£€æµ‹
        outlier_checks = detect_outliers(df_filtered)
        if outlier_checks:
            print(f"  âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å€¼: {outlier_checks}")
        
        # 5. æ•°æ®è´¨é‡è¯„åˆ†
        quality_score = calculate_data_quality_score(df_filtered)
        print(f"  - æ•°æ®è´¨é‡è¯„åˆ†: {quality_score:.1f}/100")
        
        loaded_data[week] = df_filtered
    
    return loaded_data

def detect_outliers(df, threshold=3):
    """ç®€å•å¼‚å¸¸å€¼æ£€æµ‹"""
    outliers = {}
    
    # èµ”ä»˜ç‡å¼‚å¸¸
    if 'reported_claim_payment_yuan' in df.columns and 'matured_premium_yuan' in df.columns:
        loss_ratio = df['reported_claim_payment_yuan'] / (df['matured_premium_yuan'] + 1)
        high_loss_ratio = loss_ratio > threshold
        if high_loss_ratio.any():
            outliers['é«˜èµ”ä»˜ç‡'] = high_loss_ratio.sum()
    
    # ä¿è´¹å¼‚å¸¸
    if 'signed_premium_yuan' in df.columns:
        premium = df['signed_premium_yuan']
        q99 = premium.quantile(0.99)
        high_premium = premium > q99 * 10  # è¶…è¿‡99åˆ†ä½æ•°10å€
        if high_premium.any():
            outliers['å¼‚å¸¸é«˜ä¿è´¹'] = high_premium.sum()
    
    return outliers

def calculate_data_quality_score(df):
    """è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†"""
    score = 100.0
    
    # ç¼ºå¤±å€¼æ‰£åˆ†
    missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
    score -= missing_ratio * 50
    
    # å¼‚å¸¸å€¼æ‰£åˆ†
    outliers = detect_outliers(df)
    outlier_ratio = sum(outliers.values()) / len(df) if outliers else 0
    score -= outlier_ratio * 30
    
    return max(0, min(100, score))
```

### Step 4: å½“å‘¨å€¼è®¡ç®— (V2.0æ ¸å¿ƒ)

```python
def calculate_weekly_values(loaded_data, analysis_weeks):
    """è®¡ç®—å½“å‘¨å‘ç”Ÿå€¼"""
    weekly_data = {}
    
    for week in analysis_weeks:
        if week not in loaded_data:
            print(f"âš ï¸  ç¬¬{week}å‘¨æ•°æ®ç¼ºå¤±ï¼Œè·³è¿‡å½“å‘¨å€¼è®¡ç®—")
            continue
            
        current_df = loaded_data[week]
        
        # è·å–å‰ä¸€å‘¨æ•°æ®
        previous_week = week - 1
        previous_df = loaded_data.get(previous_week)
        
        if previous_df is None:
            print(f"âš ï¸  ç¬¬{previous_week}å‘¨æ•°æ®ç¼ºå¤±ï¼Œç¬¬{week}å‘¨å½“å‘¨å€¼æ ‡è®°ä¸ºN/A")
            weekly_values = None
        else:
            # è®¡ç®—å½“å‘¨å‘ç”Ÿå€¼
            weekly_values = calculate_weekly_metrics(current_df, previous_df)
        
        # åˆå¹¶æ•°æ®
        weekly_data[week] = {
            'cumulative_data': current_df,
            'weekly_values': weekly_values,
            'previous_available': previous_df is not None
        }
    
    return weekly_data

def calculate_weekly_metrics(current_df, previous_df):
    """è®¡ç®—å…·ä½“å½“å‘¨æŒ‡æ ‡"""
    
    # æŒ‰ä¸»è¦ç»´åº¦åˆ†ç»„è®¡ç®—
    key_dimensions = ['second_level_organization', 'third_level_organization', 'business_type_category']
    
    # æ±‡æ€»çº§åˆ«è®¡ç®—
    current_summary = current_df.groupby('policy_year').agg({
        'signed_premium_yuan': 'sum',
        'matured_premium_yuan': 'sum',
        'reported_claim_payment_yuan': 'sum',
        'claim_case_count': 'sum',
        'policy_count': 'sum'
    }).reset_index()
    
    previous_summary = previous_df.groupby('policy_year').agg({
        'signed_premium_yuan': 'sum',
        'matured_premium_yuan': 'sum',
        'reported_claim_payment_yuan': 'sum',
        'claim_case_count': 'sum',
        'policy_count': 'sum'
    }).reset_index()
    
    # è®¡ç®—å½“å‘¨å€¼
    weekly_summary = current_summary.copy()
    for col in ['signed_premium_yuan', 'matured_premium_yuan', 'reported_claim_payment_yuan']:
        weekly_summary[col] = current_summary[col] - previous_summary[col]
    
    # è®¡ç®—æ¡ˆå‡èµ”æ¬¾
    weekly_summary['avg_claim_amount'] = (
        weekly_summary['reported_claim_payment_yuan'] / 
        weekly_summary['claim_case_count'].replace(0, 1)
    )
    
    return weekly_summary
```

### Step 5: å¹´åº¦æ•°æ®åˆ†ç»„ (V2.0ä¼˜åŒ–)

```python
def group_by_year_v2(weekly_data):
    """V2.0å¹´åº¦æ•°æ®åˆ†ç»„"""
    
    data_by_year = {
        '2024': {'weekly': {}, 'cumulative': {}},
        '2025': {'weekly': {}, 'cumulative': {}}
    }
    
    for week, data in weekly_data.items():
        current_df = data['cumulative_data']
        weekly_values = data['weekly_values']
        
        # åˆ†åˆ«å¤„ç†2024å’Œ2025ä¿å•å¹´åº¦
        for year in ['2024', '2025']:
            year_cumulative = current_df[current_df['policy_year'] == year]
            
            if len(year_cumulative) > 0:
                data_by_year[year]['cumulative'][week] = year_cumulative
                
                # æ·»åŠ å½“å‘¨å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
                if weekly_values is not None:
                    year_weekly = weekly_values[weekly_values['policy_year'] == year]
                    if len(year_weekly) > 0:
                        data_by_year[year]['weekly'][week] = year_weekly
    
    return data_by_year
```

### Step 6: V2.0è¾“å‡ºæ ¼å¼ (å¢å¼º)

```python
def generate_v2_output(data_by_year, analysis_params, quality_metrics):
    """ç”ŸæˆV2.0è¾“å‡ºæ ¼å¼"""
    
    import json
    from datetime import datetime
    
    # æœºæ„ç»Ÿè®¡
    all_orgs = extract_organizations(data_by_year)
    
    # æ•°æ®è´¨é‡è¯„ä¼°
    quality_report = generate_quality_report(data_by_year, quality_metrics)
    
    output = {
        "version": "2.0",
        "generated_at": datetime.now().isoformat(),
        "analysis_period": {
            "start_week": analysis_params['start_week'],
            "end_week": analysis_params['end_week'],
            "lookback_weeks": analysis_params['lookback_weeks'],
            "weeks_analyzed": analysis_params['analysis_weeks']
        },
        "data_quality": quality_report,
        "organizations": all_orgs,
        "data_summary": {
            year: {
                "cumulative_weeks": len(data_by_year[year]['cumulative']),
                "weekly_weeks": len(data_by_year[year]['weekly']),
                "total_cumulative_rows": sum(
                    len(df) for df in data_by_year[year]['cumulative'].values()
                ),
                "total_weekly_rows": sum(
                    len(df) for df in data_by_year[year]['weekly'].values()
                ) if data_by_year[year]['weekly'] else 0
            }
            for year in ['2024', '2025']
            if any(data_by_year[year]['cumulative'].values())
        },
        "weekly_calculation_status": {
            "enabled": enable_weekly_calculation,
            "successful_weeks": len([
                week for year in ['2024', '2025']
                for week in data_by_year[year]['weekly'].keys()
            ]),
            "failed_weeks": len(analysis_params.get('missing_weeks', []))
        },
        "features_enabled": {
            "weekly_calculation": enable_weekly_calculation,
            "auto_cycle_detection": auto_detect_cycles,
            "data_validation": validate_data,
            "outlier_detection": True
        }
    }
    
    return output
```

## ä½¿ç”¨ç¤ºä¾‹ (V2.0)

```python
# ç¤ºä¾‹1: å…¨è‡ªåŠ¨æ¨¡å¼ (æ¨è)
insurance-data-loader-v2(
    target_week=44,           # ç›®æ ‡å‘¨
    lookback_weeks=5,         # å›æº¯5å‘¨è¶‹åŠ¿
    enable_weekly_calculation=True,  # å¯ç”¨å½“å‘¨å€¼è®¡ç®—
    auto_detect_cycles=True   # å¯ç”¨æ™ºèƒ½å‘¨æœŸæ£€æµ‹
)

# ç¤ºä¾‹2: æ‰‹åŠ¨æŒ‡å®šå‘¨æœŸ (çµæ´»åˆ†æ)
insurance-data-loader-v2(
    start_week=35,            # æ‰‹åŠ¨æŒ‡å®šèµ·å§‹å‘¨
    end_week=39,              # æ‰‹åŠ¨æŒ‡å®šç»“æŸå‘¨
    enable_weekly_calculation=True,
    validate_data=True,       # ä¸¥æ ¼æ•°æ®éªŒè¯
    tolerance_missing=0.1     # 10%ç¼ºå¤±å®¹å¿åº¦
)

# ç¤ºä¾‹3: å¿«é€Ÿæ¨¡å¼ (å½“å‘¨åˆ†æ)
insurance-data-loader-v2(
    target_week=44,
    lookback_weeks=2,         # åªåˆ†ææœ€è¿‘2å‘¨
    enable_weekly_calculation=False,  # ç¦ç”¨å½“å‘¨å€¼è®¡ç®—
    auto_detect_cycles=True
)
```

## V2.0å¢å¼ºé”™è¯¯å¤„ç†

```python
def handle_loading_errors(load_errors, tolerance_threshold=0.3):
    """å¢å¼ºé”™è¯¯å¤„ç†"""
    
    error_count = len(load_errors)
    total_attempts = len(weeks_to_load)
    error_ratio = error_count / total_attempts if total_attempts > 0 else 0
    
    if error_ratio > tolerance_threshold:
        raise RuntimeError(f"æ•°æ®åŠ è½½å¤±è´¥ç‡{error_ratio:.1%}è¶…è¿‡å®¹å¿åº¦{tolerance_threshold:.1%}")
    
    elif error_count > 0:
        print(f"âš ï¸  æ•°æ®åŠ è½½éƒ¨åˆ†å¤±è´¥ï¼Œå¤±è´¥ç‡{error_ratio:.1%}:")
        for error in load_errors:
            print(f"  - {error}")
        print("  ç»§ç»­åˆ†æï¼Œä½†éƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")
    
    else:
        print("âœ… æ‰€æœ‰æ•°æ®åŠ è½½æˆåŠŸ")
```

## V2.0æ•°æ®ç¼“å­˜æœºåˆ¶

```python
def save_v2_cache(data_by_year, output, target_week):
    """ä¿å­˜V2.0æ•°æ®ç¼“å­˜"""
    import pickle
    from pathlib import Path
    
    cache_file = Path("å¤„ç†å/") / f".cache_v2_week_{target_week}.pkl"
    
    cache_data = {
        'version': '2.0',
        'data_by_year': data_by_year,
        'metadata': output,
        'weekly_calculation_enabled': enable_weekly_calculation,
        'analysis_period': {
            'start_week': output['analysis_period']['start_week'],
            'end_week': output['analysis_period']['end_week']
        }
    }
    
    with open(cache_file, 'wb') as f:
        pickle.dump(cache_data, f)
    
    print(f"\nğŸ’¾ V2.0æ•°æ®å·²ç¼“å­˜åˆ°: {cache_file}")
    print("   åç»­V2.0æŠ€èƒ½å¯è¯»å–æ­¤æ–‡ä»¶è·å–å®Œæ•´æ•°æ®")
    
    return cache_file
```

## V2.0ä¸åç»­æŠ€èƒ½ååŒ

**è°ƒç”¨é¡ºåºä¼˜åŒ–**:
```
insurance-data-loader-v2 â†’ ç”ŸæˆåŸºç¡€æ•°æ® + å½“å‘¨å€¼
    â†“
insurance-kpi-calculator-v2 â†’ ä½¿ç”¨å½“å‘¨å€¼è®¡ç®—è¶‹åŠ¿KPI
    â†“
insurance-loss-trend-tracker â†’ åŸºäºå½“å‘¨å€¼åˆ†æè¶‹åŠ¿
    â†“  
insurance-new-energy-truck-analyzer â†’ ä¸“é¡¹åˆ†ææ–°èƒ½æºè´§è½¦
    â†“
mckinsey-business-analysis-framework â†’ éº¦è‚¯é”¡çº§æŠ¥å‘ŠåŒ…è£…
```

**æ•°æ®æµè½¬**:
- V2.0æ•°æ®åŠ è½½å™¨æä¾›**å½“å‘¨å‘ç”Ÿå€¼**
- è¶‹åŠ¿è¿½è¸ªå™¨ä½¿ç”¨å½“å‘¨å€¼è¿›è¡Œ**å¼‚å¸¸æ³¢åŠ¨æ£€æµ‹**
- æ–°èƒ½æºè´§è½¦åˆ†æå™¨è·å¾—**ä¸“é¡¹æ•°æ®å­é›†**
- éº¦è‚¯é”¡æ¡†æ¶ç¡®ä¿**æŠ¥å‘Šä¸“ä¸šæ°´å‡†**

## V2.0è´¨é‡æ‰¿è¯º

**æ•°æ®è´¨é‡**:
- âœ… å®Œæ•´æ€§éªŒè¯: 27ä¸ªå¿…éœ€å­—æ®µæ£€æŸ¥
- âœ… å¼‚å¸¸å€¼æ£€æµ‹: è‡ªåŠ¨è¯†åˆ«æ˜æ˜¾å¼‚å¸¸æ•°æ®
- âœ… ä¸€è‡´æ€§æ£€æŸ¥: è·¨å‘¨æ•°æ®é€»è¾‘ä¸€è‡´æ€§
- âœ… ç¼ºå¤±å€¼å¤„ç†: æ™ºèƒ½æ’å€¼å’Œæ ‡è®°

**åˆ†æè´¨é‡**:
- âœ… å‘¨æœŸæ™ºèƒ½æ¨è: åŸºäºæ•°æ®å®Œæ•´æ€§
- âœ… å½“å‘¨å€¼å‡†ç¡®è®¡ç®—: å·®å€¼æ³•+å®¹é”™å¤„ç†
- âœ… å¤šç»´åº¦éªŒè¯: æœºæ„ã€ä¸šåŠ¡ã€æ—¶é—´ä¸‰ç»´æ ¡éªŒ
- âœ… å¯è¿½æº¯æ€§: å®Œæ•´çš„æ•°æ®è¡€ç¼˜è®°å½•

**ä½¿ç”¨ä½“éªŒ**:
- âœ… ä¸€é”®åˆ†æ: å…¨è‡ªåŠ¨æ¨¡å¼ï¼Œé›¶é…ç½®
- âœ… çµæ´»å®šåˆ¶: æ”¯æŒæ‰‹åŠ¨å‚æ•°è¦†ç›–
- âœ… é”™è¯¯å‹å¥½: è¯¦ç»†é”™è¯¯ä¿¡æ¯å’Œè§£å†³å»ºè®®
- âœ… æ€§èƒ½ä¼˜åŒ–: æ™ºèƒ½æ•°æ®ç¼“å­˜å’Œå¢é‡åŠ è½½