---
name: insurance-data-loader
description: è½¦é™©å‘¨æŠ¥æ•°æ®åŠ è½½å™¨ - åŠ è½½ç›®æ ‡å‘¨åŠå†å²å‘¨æ•°æ®ï¼ŒæŒ‰å¹´åº¦åˆ†ç»„ï¼Œè¿‡æ»¤æœ¬éƒ¨æœºæ„
---

# insurance-data-loader

## ä½¿ç”¨åœºæ™¯

æ¯æ¬¡ç”Ÿæˆè½¦é™©å‘¨æŠ¥çš„ç¬¬ä¸€æ­¥ï¼Œç”¨äºåŠ è½½å’Œé¢„å¤„ç†æ•°æ®ã€‚

## åŠŸèƒ½è¯´æ˜

æœ¬skillè´Ÿè´£ï¼š
1. åŠ è½½ç›®æ ‡å‘¨åŠå‰Nå‘¨çš„CSVæ–‡ä»¶
2. éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼ˆå¿…é¡»åŒ…å«27ä¸ªå­—æ®µï¼‰
3. æŒ‰ä¿é™©èµ·æœŸå¹´åº¦åˆ†ç»„ï¼ˆ2024ä¿å• / 2025ä¿å•ï¼‰
4. è¿‡æ»¤æ‰"æœ¬éƒ¨"æœºæ„ï¼ˆä¸‰çº§æœºæ„ = 'æœ¬éƒ¨'ï¼‰
5. æå–æ‰€æœ‰å”¯ä¸€çš„äºŒçº§æœºæ„å’Œä¸‰çº§æœºæ„åç§°åˆ—è¡¨

## è¾“å…¥å‚æ•°

- `target_week`: ç›®æ ‡å‘¨æ¬¡ï¼ˆå¦‚ 44ï¼‰
- `data_folder`: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆé»˜è®¤ `å¤„ç†å/`ï¼‰
- `lookback_weeks`: å›æº¯å‘¨æ•°ï¼ˆé»˜è®¤ 5ï¼Œç”¨äºè¶‹åŠ¿åˆ†æï¼‰

## è¾“å‡ºæ ¼å¼

è¾“å‡ºä¸€ä¸ªJSONç»“æ„ï¼ŒåŒ…å«ï¼š
- æŒ‰å¹´åº¦åˆ†ç»„çš„DataFrameå­—å…¸
- æœºæ„åˆ—è¡¨
- æ•°æ®è´¨é‡å…ƒä¿¡æ¯

## æ‰§è¡Œæ­¥éª¤

### Step 1: éªŒè¯è¾“å…¥å‚æ•°

```python
# å¿…é¡»æä¾›çš„å‚æ•°
if not target_week:
    raise ValueError("å¿…é¡»æŒ‡å®štarget_weekå‚æ•°")

# è®¾ç½®é»˜è®¤å€¼
data_folder = data_folder or "å¤„ç†å/"
lookback_weeks = lookback_weeks or 5
```

### Step 2: æŸ¥æ‰¾å¹¶åŠ è½½CSVæ–‡ä»¶

```python
import pandas as pd
from pathlib import Path
import os

# è®¡ç®—éœ€è¦åŠ è½½çš„å‘¨æ¬¡åˆ—è¡¨
weeks_to_load = list(range(target_week - lookback_weeks + 1, target_week + 1))
# ç¤ºä¾‹ï¼štarget_week=44, lookback_weeks=5 â†’ [40, 41, 42, 43, 44]

# æŸ¥æ‰¾æ–‡ä»¶
data_path = Path(data_folder)
loaded_files = {}
missing_weeks = []

for week in weeks_to_load:
    # æ”¯æŒä¸¤ç§å‘½åæ ¼å¼ï¼š
    # - 2024ä¿å•ç¬¬XXå‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv
    # - 2025ä¿å•ç¬¬XXå‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv

    # å…ˆå°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
    pattern = f"*ä¿å•ç¬¬{week}å‘¨å˜åŠ¨æˆæœ¬æ˜ç»†è¡¨.csv"
    matching_files = list(data_path.glob(pattern))

    if matching_files:
        for file in matching_files:
            print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {file.name}")
            try:
                df = pd.read_csv(file, encoding='utf-8-sig')
                loaded_files[week] = loaded_files.get(week, []) + [df]
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file.name}: {e}")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°ç¬¬{week}å‘¨çš„æ•°æ®æ–‡ä»¶")
        missing_weeks.append(week)

# æ±‡æ€»åŠ è½½æƒ…å†µ
print(f"\nğŸ“Š æ•°æ®åŠ è½½æ±‡æ€»:")
print(f"  - ç›®æ ‡å‘¨: ç¬¬{target_week}å‘¨")
print(f"  - å›æº¯å‘¨æ•°: {lookback_weeks}å‘¨")
print(f"  - æˆåŠŸåŠ è½½: {len(loaded_files)}ä¸ªå‘¨æ¬¡")
if missing_weeks:
    print(f"  - ç¼ºå¤±å‘¨æ¬¡: {missing_weeks}")
```

### Step 3: éªŒè¯æ•°æ®å®Œæ•´æ€§

```python
# å¿…é¡»çš„27ä¸ªå­—æ®µ
REQUIRED_FIELDS = [
    # 18ä¸ªç­›é€‰ç»´åº¦å­—æ®µ
    'snapshot_date',
    'policy_start_year',
    'business_type_category',
    'chengdu_branch',
    'second_level_organization',
    'third_level_organization',
    'customer_category_3',
    'insurance_type',
    'is_new_energy_vehicle',
    'coverage_type',
    'is_transferred_vehicle',
    'renewal_status',
    'vehicle_insurance_grade',
    'highway_risk_grade',
    'large_truck_score',
    'small_truck_score',
    'terminal_source',
    # 9ä¸ªç»å¯¹å€¼å­—æ®µ
    'signed_premium_yuan',
    'matured_premium_yuan',
    'policy_count',
    'claim_case_count',
    'reported_claim_payment_yuan',
    'expense_amount_yuan',
    'commercial_premium_before_discount_yuan',
    'premium_plan_yuan',
    'marginal_contribution_amount_yuan',
    'week_number'
]

# éªŒè¯æ¯ä¸ªåŠ è½½çš„æ–‡ä»¶
for week, dfs in loaded_files.items():
    for df in dfs:
        missing_fields = set(REQUIRED_FIELDS) - set(df.columns)
        if missing_fields:
            print(f"âŒ ç¬¬{week}å‘¨æ•°æ®ç¼ºå°‘å­—æ®µ: {missing_fields}")
        else:
            print(f"âœ… ç¬¬{week}å‘¨æ•°æ®å­—æ®µå®Œæ•´")
```

### Step 4: è¿‡æ»¤æœ¬éƒ¨æœºæ„

```python
# è¿‡æ»¤æ‰ä¸‰çº§æœºæ„ä¸º"æœ¬éƒ¨"çš„æ•°æ®
for week, dfs in loaded_files.items():
    for i, df in enumerate(dfs):
        original_count = len(df)
        df_filtered = df[df['third_level_organization'] != 'æœ¬éƒ¨'].copy()
        filtered_count = len(df_filtered)

        print(f"ç¬¬{week}å‘¨: è¿‡æ»¤å‰{original_count}è¡Œï¼Œè¿‡æ»¤å{filtered_count}è¡Œï¼Œå‰”é™¤{original_count - filtered_count}è¡Œæœ¬éƒ¨æ•°æ®")

        loaded_files[week][i] = df_filtered
```

### Step 5: æŒ‰å¹´åº¦åˆ†ç»„

```python
# æŒ‰ä¿é™©èµ·æœŸå¹´åº¦åˆ†ç»„
data_by_year = {
    '2024': {},
    '2025': {}
}

for week, dfs in loaded_files.items():
    for df in dfs:
        # ä»policy_start_yearå­—æ®µæå–å¹´ä»½
        # å¯èƒ½çš„æ ¼å¼ï¼š2024, 2025, "2024", "2025"
        df['year'] = df['policy_start_year'].astype(str).str.extract(r'(202[45])')[0]

        # åˆ†ç»„
        for year in ['2024', '2025']:
            year_data = df[df['year'] == year].copy()
            if len(year_data) > 0:
                if week not in data_by_year[year]:
                    data_by_year[year][week] = []
                data_by_year[year][week].append(year_data)
                print(f"ğŸ“‹ {year}ä¿å•ç¬¬{week}å‘¨: {len(year_data)}è¡Œæ•°æ®")

# åˆå¹¶åŒä¸€å‘¨çš„å¤šä¸ªæ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
for year in ['2024', '2025']:
    for week in data_by_year[year].keys():
        if len(data_by_year[year][week]) > 1:
            print(f"âš ï¸  {year}ä¿å•ç¬¬{week}å‘¨æœ‰å¤šä¸ªæ–‡ä»¶ï¼Œæ­£åœ¨åˆå¹¶...")
            data_by_year[year][week] = pd.concat(data_by_year[year][week], ignore_index=True)
        else:
            data_by_year[year][week] = data_by_year[year][week][0]
```

### Step 6: æå–æœºæ„åˆ—è¡¨

```python
# æ”¶é›†æ‰€æœ‰æœºæ„åç§°ï¼ˆå»é‡ï¼‰
all_second_orgs = set()
all_third_orgs = set()

for year in ['2024', '2025']:
    for week, df in data_by_year[year].items():
        all_second_orgs.update(df['second_level_organization'].unique())
        all_third_orgs.update(df['third_level_organization'].unique())

# ç§»é™¤ç©ºå€¼å’Œæœ¬éƒ¨
all_second_orgs = sorted([org for org in all_second_orgs if pd.notna(org) and org != 'æœ¬éƒ¨'])
all_third_orgs = sorted([org for org in all_third_orgs if pd.notna(org) and org != 'æœ¬éƒ¨'])

print(f"\nğŸ¢ æœºæ„åˆ—è¡¨:")
print(f"  - äºŒçº§æœºæ„æ•°é‡: {len(all_second_orgs)}")
print(f"  - ä¸‰çº§æœºæ„æ•°é‡: {len(all_third_orgs)}")
print(f"  - ä¸‰çº§æœºæ„: {', '.join(all_third_orgs)}")
```

### Step 7: ç”Ÿæˆè¾“å‡ºæŠ¥å‘Š

```python
# ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
print("\n" + "="*60)
print("ğŸ“Š æ•°æ®åŠ è½½å®ŒæˆæŠ¥å‘Š")
print("="*60)

print(f"\nâœ… æˆåŠŸåŠ è½½ {len(weeks_to_load) - len(missing_weeks)}/{len(weeks_to_load)} ä¸ªå‘¨æ¬¡")

for year in ['2024', '2025']:
    if data_by_year[year]:
        print(f"\n{year}ä¿å•æ•°æ®:")
        for week in sorted(data_by_year[year].keys()):
            df = data_by_year[year][week]
            print(f"  - ç¬¬{week}å‘¨: {len(df):,}è¡Œæ•°æ®")
            print(f"    æ»¡æœŸä¿è´¹åˆè®¡: {df['matured_premium_yuan'].sum() / 10000:.2f}ä¸‡å…ƒ")
    else:
        print(f"\n{year}ä¿å•æ•°æ®: æ— ")

print(f"\nğŸ¢ æœºæ„ç»Ÿè®¡:")
print(f"  - äºŒçº§æœºæ„: {len(all_second_orgs)}ä¸ª")
print(f"  - ä¸‰çº§æœºæ„: {len(all_third_orgs)}ä¸ªï¼ˆå·²æ’é™¤æœ¬éƒ¨ï¼‰")

if missing_weeks:
    print(f"\nâš ï¸  ç¼ºå¤±å‘¨æ¬¡: {missing_weeks}")
    print(f"   è¯´æ˜: è¿™äº›å‘¨æ¬¡çš„æ•°æ®å°†æ— æ³•è¿›è¡Œç¯æ¯”åˆ†æ")

print("\n" + "="*60)
```

### Step 8: è¾“å‡ºæ•°æ®ç»“æ„

```python
# å°†æ•°æ®ä¿å­˜åˆ°å…¨å±€å˜é‡ï¼Œä¾›åç»­skillä½¿ç”¨
# æ³¨æ„ï¼šClaude Codeçš„skillä¹‹é—´é€šè¿‡è¾“å‡ºæ–‡æœ¬å’Œæ–‡ä»¶å…±äº«æ•°æ®

# è¾“å‡ºä¸€ä¸ªJSONæ ¼å¼çš„æ•°æ®è·¯å¾„æ˜ å°„
import json

output = {
    "target_week": target_week,
    "lookback_weeks": lookback_weeks,
    "data_quality": "å®Œæ•´" if not missing_weeks else "éƒ¨åˆ†ç¼ºå¤±",
    "missing_weeks": missing_weeks,
    "organizations": {
        "second_level": all_second_orgs,
        "third_level": all_third_orgs
    },
    "loaded_years": list(data_by_year.keys()),
    "data_summary": {
        year: {
            "weeks": list(data_by_year[year].keys()),
            "total_rows": sum(len(df) for df in data_by_year[year].values())
        }
        for year in ['2024', '2025'] if data_by_year[year]
    }
}

print("\nğŸ“¦ æ•°æ®åŠ è½½å™¨è¾“å‡º (JSON):")
print(json.dumps(output, ensure_ascii=False, indent=2))

# ä¿å­˜æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œä¾›åç»­skillè¯»å–
import pickle
cache_file = Path(data_folder) / f".cache_week_{target_week}.pkl"
with open(cache_file, 'wb') as f:
    pickle.dump({
        'data_by_year': data_by_year,
        'metadata': output
    }, f)

print(f"\nğŸ’¾ æ•°æ®å·²ç¼“å­˜åˆ°: {cache_file}")
print("   åç»­skillå¯é€šè¿‡è¯»å–æ­¤æ–‡ä»¶è·å–æ•°æ®")
```

## ä½¿ç”¨ç¤ºä¾‹

```python
# ç¤ºä¾‹1: åŠ è½½ç¬¬44å‘¨åŠå‰5å‘¨æ•°æ®
insurance-data-loader(
    target_week=44,
    data_folder="å¤„ç†å/",
    lookback_weeks=5
)

# ç¤ºä¾‹2: åªåŠ è½½ç›®æ ‡å‘¨åŠå‰2å‘¨ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
insurance-data-loader(
    target_week=44,
    lookback_weeks=2
)
```

## é”™è¯¯å¤„ç†

- å¦‚æœç›®æ ‡å‘¨æ•°æ®ç¼ºå¤±ï¼ŒæŠ¥é”™å¹¶ç»ˆæ­¢
- å¦‚æœå†å²å‘¨æ•°æ®éƒ¨åˆ†ç¼ºå¤±ï¼Œç»™å‡ºè­¦å‘Šä½†ç»§ç»­æ‰§è¡Œ
- å¦‚æœæ•°æ®å­—æ®µä¸å®Œæ•´ï¼ŒæŠ¥é”™å¹¶åˆ—å‡ºç¼ºå¤±å­—æ®µ
- å¦‚æœæ–‡ä»¶è¯»å–å¤±è´¥ï¼ŒæŠ¥é”™å¹¶æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯

## ä¸‹ä¸€æ­¥

æ•°æ®åŠ è½½å®Œæˆåï¼Œè°ƒç”¨ï¼š
- `insurance-kpi-calculator`: è®¡ç®—KPIæŒ‡æ ‡
- `insurance-org-profiler`: ç”Ÿæˆæœºæ„ç”»åƒ
